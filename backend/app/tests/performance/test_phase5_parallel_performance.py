"""
Phase 5 Parallel Performance Tests - ãƒ•ã‚§ãƒ¼ã‚º5ä¸¦åˆ—å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
ç”»åƒç”Ÿæˆä¸¦åˆ—å‡¦ç†ã®æ€§èƒ½æ¤œè¨¼ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šã€ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡è©•ä¾¡
"""

import asyncio
import time
import psutil
import pytest
from datetime import datetime
from typing import List, Dict, Any
from unittest.mock import AsyncMock, MagicMock

from app.agents.phase5_image import Phase5ImageAgent
from app.services.parallel_quality_orchestrator import ParallelQualityOrchestrator
from app.core.config.parallel_processing import parallel_processing_config


class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.cpu_usage_start = None
        self.cpu_usage_end = None
        self.memory_usage_start = None
        self.memory_usage_end = None
        self.throughput_data = []
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.start_time = time.time()
        self.cpu_usage_start = psutil.cpu_percent(interval=None)
        self.memory_usage_start = psutil.virtual_memory().percent
        
    def end_monitoring(self):
        """ç›£è¦–çµ‚äº†"""
        self.end_time = time.time()
        self.cpu_usage_end = psutil.cpu_percent(interval=None)
        self.memory_usage_end = psutil.virtual_memory().percent
    
    def get_execution_time(self) -> float:
        """å®Ÿè¡Œæ™‚é–“å–å¾—"""
        return self.end_time - self.start_time if self.end_time else 0
    
    def get_cpu_usage_change(self) -> float:
        """CPUä½¿ç”¨ç‡å¤‰åŒ–é‡"""
        return self.cpu_usage_end - self.cpu_usage_start if self.cpu_usage_end else 0
    
    def get_memory_usage_change(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡å¤‰åŒ–é‡"""
        return self.memory_usage_end - self.memory_usage_start if self.memory_usage_end else 0
    
    def add_throughput_point(self, completed_items: int, elapsed_time: float):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿è¿½åŠ """
        throughput = completed_items / elapsed_time if elapsed_time > 0 else 0
        self.throughput_data.append({
            "completed_items": completed_items,
            "elapsed_time": elapsed_time,
            "throughput": throughput
        })
    
    def get_average_throughput(self) -> float:
        """å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—"""
        if not self.throughput_data:
            return 0
        return sum(data["throughput"] for data in self.throughput_data) / len(self.throughput_data)


@pytest.fixture
def mock_phase5_agent():
    """ãƒ¢ãƒƒã‚¯Phase5ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    agent = MagicMock(spec=Phase5ImageAgent)
    
    # ä¸¦åˆ—å‡¦ç†è¨­å®š
    agent.max_concurrent_generations = 5
    agent.semaphore = asyncio.Semaphore(5)
    
    # ãƒ¢ãƒƒã‚¯ç”»åƒç”Ÿæˆé–¢æ•°
    async def mock_generate_image(task):
        # å®Ÿéš›ã®ç”»åƒç”Ÿæˆæ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆ1-3ç§’ï¼‰
        await asyncio.sleep(0.1 + (hash(str(task)) % 20) * 0.01)
        return {
            "success": True,
            "image_url": f"mock_image_{hash(str(task))}.jpg",
            "generation_time": 2.5,
            "quality_score": 0.85
        }
    
    agent._generate_single_image = mock_generate_image
    return agent


@pytest.fixture
def test_scene_data():
    """ãƒ†ã‚¹ãƒˆç”¨ã‚·ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿"""
    return [
        {
            "scene_id": f"scene_{i}",
            "prompt": f"ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ{i}",
            "style": "manga",
            "character": f"character_{i % 3}",  # 3ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’å¾ªç’°
            "background": f"background_{i % 5}" # 5èƒŒæ™¯ã‚’å¾ªç’°
        }
        for i in range(20)  # 20ã‚·ãƒ¼ãƒ³ã§è² è·ãƒ†ã‚¹ãƒˆ
    ]


class TestPhase5ParallelPerformance:
    """ãƒ•ã‚§ãƒ¼ã‚º5ä¸¦åˆ—å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_sequential_vs_parallel_image_generation(self, test_scene_data):
        """é †æ¬¡ vs ä¸¦åˆ—ç”»åƒç”Ÿæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
        print(f"\nğŸ§ª Sequential vs Parallel Performance Test")
        print(f"Test scenes: {len(test_scene_data)}")
        
        # é †æ¬¡å‡¦ç†ãƒ†ã‚¹ãƒˆ
        sequential_metrics = PerformanceMetrics()
        sequential_metrics.start_monitoring()
        
        sequential_results = []
        for scene in test_scene_data:
            # é †æ¬¡ç”»åƒç”Ÿæˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            await asyncio.sleep(0.15)  # é †æ¬¡å‡¦ç†ã®é…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            sequential_results.append({
                "scene_id": scene["scene_id"],
                "success": True,
                "processing_time": 0.15
            })
        
        sequential_metrics.end_monitoring()
        
        # ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
        parallel_metrics = PerformanceMetrics()
        parallel_metrics.start_monitoring()
        
        # ã‚»ãƒãƒ•ã‚©ã§åˆ¶å¾¡ã•ã‚ŒãŸä¸¦åˆ—å®Ÿè¡Œ
        semaphore = asyncio.Semaphore(5)
        
        async def process_scene_parallel(scene):
            async with semaphore:
                await asyncio.sleep(0.12)  # ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–åŠ¹æœ
                return {
                    "scene_id": scene["scene_id"], 
                    "success": True,
                    "processing_time": 0.12
                }
        
        parallel_tasks = [process_scene_parallel(scene) for scene in test_scene_data]
        parallel_results = await asyncio.gather(*parallel_tasks)
        
        parallel_metrics.end_monitoring()
        
        # æ€§èƒ½åˆ†æ
        sequential_time = sequential_metrics.get_execution_time()
        parallel_time = parallel_metrics.get_execution_time()
        performance_improvement = (sequential_time - parallel_time) / sequential_time
        
        # æ¤œè¨¼
        assert len(sequential_results) == len(parallel_results) == len(test_scene_data)
        assert parallel_time < sequential_time
        assert performance_improvement >= 0.6  # 60%ä»¥ä¸Šã®æ”¹å–„ã‚’æœŸå¾…
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        print(f"ğŸ“Š Performance Results:")
        print(f"   Sequential Time: {sequential_time:.2f}s")
        print(f"   Parallel Time: {parallel_time:.2f}s")
        print(f"   Performance Improvement: {performance_improvement:.1%}")
        print(f"   Theoretical Max Speedup: {min(5, len(test_scene_data))}x")
        print(f"   Actual Speedup: {sequential_time/parallel_time:.1f}x")
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_throughput_scalability(self, test_scene_data):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        print(f"\nğŸš€ Throughput Scalability Test")
        
        # ç•°ãªã‚‹ä¸¦åˆ—åº¦ã§ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
        concurrency_levels = [1, 2, 3, 5, 8]
        throughput_results = []
        
        for concurrency in concurrency_levels:
            print(f"   Testing concurrency: {concurrency}")
            
            metrics = PerformanceMetrics()
            metrics.start_monitoring()
            
            semaphore = asyncio.Semaphore(concurrency)
            
            async def process_with_concurrency(scene):
                async with semaphore:
                    processing_time = 0.1 + (hash(scene["scene_id"]) % 10) * 0.01
                    await asyncio.sleep(processing_time)
                    return {"scene_id": scene["scene_id"], "success": True}
            
            tasks = [process_with_concurrency(scene) for scene in test_scene_data]
            results = await asyncio.gather(*tasks)
            
            metrics.end_monitoring()
            
            execution_time = metrics.get_execution_time()
            throughput = len(results) / execution_time if execution_time > 0 else 0
            
            throughput_results.append({
                "concurrency": concurrency,
                "execution_time": execution_time,
                "throughput": throughput,
                "cpu_usage": metrics.get_cpu_usage_change()
            })
            
            print(f"     Execution Time: {execution_time:.2f}s")
            print(f"     Throughput: {throughput:.1f} images/sec")
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ
        base_throughput = throughput_results[0]["throughput"]
        max_throughput = max(result["throughput"] for result in throughput_results)
        optimal_concurrency = max(throughput_results, key=lambda x: x["throughput"])
        
        print(f"\nğŸ“ˆ Scalability Analysis:")
        print(f"   Base Throughput (1 worker): {base_throughput:.1f} images/sec")
        print(f"   Max Throughput: {max_throughput:.1f} images/sec")
        print(f"   Optimal Concurrency: {optimal_concurrency['concurrency']} workers")
        print(f"   Scalability Factor: {max_throughput/base_throughput:.1f}x")
        
        # æ¤œè¨¼
        assert max_throughput > base_throughput * 2  # æœ€ä½2å€ã®æ”¹å–„
        assert optimal_concurrency["concurrency"] <= 8  # ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ã®ç¢ºèª
    
    @pytest.mark.asyncio  
    @pytest.mark.performance
    async def test_resource_efficiency(self, test_scene_data):
        """ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ãƒ†ã‚¹ãƒˆ"""
        print(f"\nğŸ”§ Resource Efficiency Test")
        
        # é«˜è² è·ã§ã®é•·æ™‚é–“å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        large_dataset = test_scene_data * 5  # 100ã‚·ãƒ¼ãƒ³
        
        metrics = PerformanceMetrics()
        metrics.start_monitoring()
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ç”¨
        initial_memory = psutil.virtual_memory().used
        
        semaphore = asyncio.Semaphore(5)
        processed_count = 0
        
        async def process_with_monitoring(scene):
            nonlocal processed_count
            async with semaphore:
                # å‡¦ç†æ™‚é–“ã®å¤‰å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                processing_time = 0.08 + (hash(scene["scene_id"]) % 15) * 0.01
                await asyncio.sleep(processing_time)
                
                processed_count += 1
                
                # é€²æ—ç›£è¦–
                if processed_count % 20 == 0:
                    current_memory = psutil.virtual_memory().used
                    memory_growth = (current_memory - initial_memory) / 1024 / 1024  # MB
                    elapsed = time.time() - metrics.start_time
                    current_throughput = processed_count / elapsed
                    
                    metrics.add_throughput_point(processed_count, elapsed)
                    
                    print(f"     Progress: {processed_count}/{len(large_dataset)} " +
                          f"({processed_count/len(large_dataset):.1%}) - " +
                          f"Memory Growth: {memory_growth:.1f}MB - " +
                          f"Throughput: {current_throughput:.1f} imgs/sec")
                
                return {"scene_id": scene["scene_id"], "success": True}
        
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        tasks = [process_with_monitoring(scene) for scene in large_dataset]
        results = await asyncio.gather(*tasks)
        
        metrics.end_monitoring()
        
        final_memory = psutil.virtual_memory().used
        memory_growth = (final_memory - initial_memory) / 1024 / 1024  # MB
        
        # åŠ¹ç‡åˆ†æ
        execution_time = metrics.get_execution_time()
        total_throughput = len(results) / execution_time
        average_throughput = metrics.get_average_throughput()
        cpu_efficiency = len(results) / max(metrics.get_cpu_usage_change(), 1)
        
        print(f"\nâš¡ Resource Efficiency Analysis:")
        print(f"   Total Images: {len(results)}")
        print(f"   Execution Time: {execution_time:.2f}s")
        print(f"   Final Throughput: {total_throughput:.1f} images/sec")
        print(f"   Average Throughput: {average_throughput:.1f} images/sec")
        print(f"   Memory Growth: {memory_growth:.1f}MB")
        print(f"   CPU Efficiency: {cpu_efficiency:.1f} images/cpu_percent")
        
        # åŠ¹ç‡æ€§æ¤œè¨¼
        assert len(results) == len(large_dataset)
        assert all(result["success"] for result in results)
        assert memory_growth < 100  # 100MBæœªæº€ã®ãƒ¡ãƒ¢ãƒªå¢—åŠ 
        assert total_throughput >= 8   # 8 images/secä»¥ä¸Šã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
        
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_error_resilience_performance(self, test_scene_data):
        """ã‚¨ãƒ©ãƒ¼è€æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print(f"\nğŸ›¡ï¸ Error Resilience Performance Test")
        
        # ã‚¨ãƒ©ãƒ¼ç‡ã‚’æ®µéšçš„ã«å¢—åŠ ã•ã›ã¦ãƒ†ã‚¹ãƒˆ
        error_rates = [0.0, 0.1, 0.2, 0.3]
        resilience_results = []
        
        for error_rate in error_rates:
            print(f"   Testing error rate: {error_rate:.1%}")
            
            metrics = PerformanceMetrics()
            metrics.start_monitoring()
            
            semaphore = asyncio.Semaphore(5)
            success_count = 0
            error_count = 0
            
            async def process_with_errors(scene):
                nonlocal success_count, error_count
                async with semaphore:
                    # ã‚¨ãƒ©ãƒ¼ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ç™ºç”Ÿ
                    should_error = hash(scene["scene_id"]) % 100 < error_rate * 100
                    
                    if should_error:
                        error_count += 1
                        # ã‚¨ãƒ©ãƒ¼å‡¦ç†æ™‚é–“ï¼ˆãƒªãƒˆãƒ©ã‚¤ãªã©ï¼‰
                        await asyncio.sleep(0.05)
                        return {"scene_id": scene["scene_id"], "success": False, "error": "simulated_error"}
                    else:
                        success_count += 1
                        await asyncio.sleep(0.12)
                        return {"scene_id": scene["scene_id"], "success": True}
            
            tasks = [process_with_errors(scene) for scene in test_scene_data]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            metrics.end_monitoring()
            
            execution_time = metrics.get_execution_time()
            success_rate = success_count / len(test_scene_data)
            throughput = success_count / execution_time
            
            resilience_results.append({
                "error_rate": error_rate,
                "success_rate": success_rate,
                "execution_time": execution_time,
                "throughput": throughput,
                "error_count": error_count
            })
            
            print(f"     Success Rate: {success_rate:.1%}")
            print(f"     Throughput: {throughput:.1f} images/sec")
            print(f"     Execution Time: {execution_time:.2f}s")
        
        # è€æ€§åˆ†æ
        base_throughput = resilience_results[0]["throughput"]
        
        print(f"\nğŸ” Error Resilience Analysis:")
        for result in resilience_results:
            throughput_retention = result["throughput"] / base_throughput
            print(f"   Error Rate {result['error_rate']:.1%}: " +
                  f"Throughput Retention {throughput_retention:.1%}, " +
                  f"Success Rate {result['success_rate']:.1%}")
        
        # è€æ€§æ¤œè¨¼
        assert all(result["success_rate"] >= (1 - result["error_rate"]) * 0.9 for result in resilience_results)
        assert resilience_results[-1]["throughput"] / base_throughput >= 0.5  # é«˜ã‚¨ãƒ©ãƒ¼ç‡ã§ã‚‚50%ä»¥ä¸Šã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆç¶­æŒ


if __name__ == "__main__":
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    pytest.main([
        __file__,
        "-v",
        "-m", "performance",
        "--tb=short",
        "--durations=10"
    ])