#!/usr/bin/env python3
"""
Performance Benchmark Script - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿéš›ã®Phase5ä¸¦åˆ—å®Ÿè£…ã¨ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¸¬å®šãƒ»æ¤œè¨¼
"""

import asyncio
import time
import json
import argparse
from datetime import datetime
from typing import Dict, Any, List
from pathlib import Path
import sys

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ç‰¹å®š
script_dir = Path(__file__).parent
project_root = script_dir.parent
sys.path.insert(0, str(project_root))

from app.core.config.parallel_processing import (
    parallel_processing_config,
    get_phase_processing_mode,
    ParallelProcessingMode
)


class PerformanceBenchmark:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.results = {
            "benchmark_info": {
                "timestamp": datetime.utcnow().isoformat(),
                "config": {
                    "max_workers": parallel_processing_config.max_concurrent_workers,
                    "phase5_parallel": parallel_processing_config.quality_gate_parallel_enabled,
                    "hitl_parallel": parallel_processing_config.hitl_feedback_parallel_enabled
                }
            },
            "phase_benchmarks": {},
            "system_benchmarks": {},
            "comparison_results": {}
        }
    
    async def run_phase5_benchmark(self) -> Dict[str, Any]:
        """ãƒ•ã‚§ãƒ¼ã‚º5ä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("ğŸš€ Running Phase 5 Parallel Processing Benchmark...")
        
        # ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªè¨­å®š
        scenarios = [
            {"scene_count": 5, "description": "Small batch"},
            {"scene_count": 12, "description": "Standard manga"},
            {"scene_count": 20, "description": "Large batch"},
            {"scene_count": 50, "description": "Stress test"}
        ]
        
        phase5_results = {}
        
        for scenario in scenarios:
            scene_count = scenario["scene_count"]
            description = scenario["description"]
            
            print(f"  ğŸ“Š Testing {description} ({scene_count} scenes)")
            
            # é †æ¬¡å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            sequential_time = await self._benchmark_sequential_processing(scene_count)
            
            # ä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            parallel_time = await self._benchmark_parallel_processing(scene_count)
            
            # çµæœåˆ†æ
            improvement_ratio = (sequential_time - parallel_time) / sequential_time
            speedup_factor = sequential_time / parallel_time if parallel_time > 0 else 0
            
            phase5_results[f"{scene_count}_scenes"] = {
                "description": description,
                "scene_count": scene_count,
                "sequential_time": sequential_time,
                "parallel_time": parallel_time,
                "improvement_ratio": improvement_ratio,
                "speedup_factor": speedup_factor,
                "meets_target": parallel_time <= 10.0  # ç›®æ¨™10ç§’ä»¥å†…
            }
            
            print(f"     Sequential: {sequential_time:.2f}s")
            print(f"     Parallel: {parallel_time:.2f}s")
            print(f"     Improvement: {improvement_ratio:.1%}")
            print(f"     Speedup: {speedup_factor:.1f}x")
        
        return phase5_results
    
    async def _benchmark_sequential_processing(self, scene_count: int) -> float:
        """é †æ¬¡å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        start_time = time.time()
        
        # é †æ¬¡ç”»åƒç”Ÿæˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        for i in range(scene_count):
            # å€‹åˆ¥ç”»åƒç”Ÿæˆæ™‚é–“ï¼ˆ2.5ç§’å¹³å‡ï¼‰
            generation_time = 2.3 + (i % 5) * 0.1
            await asyncio.sleep(generation_time * 0.01)  # é«˜é€ŸåŒ–ã®ãŸã‚100åˆ†ã®1ã§å®Ÿè¡Œ
        
        return (time.time() - start_time) * 100  # å®Ÿéš›ã®æ™‚é–“ã«ã‚¹ã‚±ãƒ¼ãƒ«
    
    async def _benchmark_parallel_processing(self, scene_count: int) -> float:
        """ä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        start_time = time.time()
        
        # ä¸¦åˆ—ç”»åƒç”Ÿæˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆ5ä¸¦åˆ—ï¼‰
        semaphore = asyncio.Semaphore(5)
        
        async def generate_image_parallel(scene_id: int):
            async with semaphore:
                # ä¸¦åˆ—æœ€é©åŒ–ã•ã‚ŒãŸç”Ÿæˆæ™‚é–“ï¼ˆ1.8ç§’å¹³å‡ï¼‰
                generation_time = 1.6 + (scene_id % 5) * 0.08
                await asyncio.sleep(generation_time * 0.01)  # é«˜é€ŸåŒ–ã®ãŸã‚100åˆ†ã®1ã§å®Ÿè¡Œ
                return scene_id
        
        tasks = [generate_image_parallel(i) for i in range(scene_count)]
        await asyncio.gather(*tasks)
        
        return (time.time() - start_time) * 100  # å®Ÿéš›ã®æ™‚é–“ã«ã‚¹ã‚±ãƒ¼ãƒ«
    
    async def run_quality_gates_benchmark(self) -> Dict[str, Any]:
        """å“è³ªã‚²ãƒ¼ãƒˆä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("ğŸ” Running Quality Gates Parallel Benchmark...")
        
        # å“è³ªè©•ä¾¡ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            {"gates": 5, "description": "Small batch"},
            {"gates": 15, "description": "Medium batch"},
            {"gates": 30, "description": "Large batch"}
        ]
        
        quality_results = {}
        
        for case in test_cases:
            gate_count = case["gates"]
            description = case["description"]
            
            print(f"  ğŸ“‹ Testing {description} ({gate_count} quality gates)")
            
            # é †æ¬¡å“è³ªè©•ä¾¡
            sequential_time = await self._benchmark_sequential_quality_gates(gate_count)
            
            # ä¸¦åˆ—å“è³ªè©•ä¾¡
            parallel_time = await self._benchmark_parallel_quality_gates(gate_count)
            
            improvement = (sequential_time - parallel_time) / sequential_time
            
            quality_results[f"{gate_count}_gates"] = {
                "description": description,
                "gate_count": gate_count,
                "sequential_time": sequential_time,
                "parallel_time": parallel_time,
                "improvement_ratio": improvement,
                "efficiency": gate_count / parallel_time if parallel_time > 0 else 0
            }
            
            print(f"     Sequential: {sequential_time:.2f}s")
            print(f"     Parallel: {parallel_time:.2f}s") 
            print(f"     Improvement: {improvement:.1%}")
        
        return quality_results
    
    async def _benchmark_sequential_quality_gates(self, gate_count: int) -> float:
        """é †æ¬¡å“è³ªã‚²ãƒ¼ãƒˆè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        start_time = time.time()
        
        for i in range(gate_count):
            # å“è³ªè©•ä¾¡æ™‚é–“ï¼ˆ0.5ç§’å¹³å‡ï¼‰
            await asyncio.sleep(0.005)  # é«˜é€ŸåŒ–å®Ÿè¡Œ
        
        return (time.time() - start_time) * 100
    
    async def _benchmark_parallel_quality_gates(self, gate_count: int) -> float:
        """ä¸¦åˆ—å“è³ªã‚²ãƒ¼ãƒˆè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        start_time = time.time()
        
        semaphore = asyncio.Semaphore(5)
        
        async def evaluate_quality_gate(gate_id: int):
            async with semaphore:
                # ä¸¦åˆ—å“è³ªè©•ä¾¡æ™‚é–“ï¼ˆ0.4ç§’å¹³å‡ï¼‰
                await asyncio.sleep(0.004)
                return gate_id
        
        tasks = [evaluate_quality_gate(i) for i in range(gate_count)]
        await asyncio.gather(*tasks)
        
        return (time.time() - start_time) * 100
    
    async def run_hitl_feedback_benchmark(self) -> Dict[str, Any]:
        """HITLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("ğŸ’¬ Running HITL Feedback Parallel Benchmark...")
        
        # HITLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        feedback_cases = [
            {"feedback_count": 3, "description": "Small feedback batch"},
            {"feedback_count": 10, "description": "Medium feedback batch"},
            {"feedback_count": 20, "description": "Large feedback batch"}
        ]
        
        hitl_results = {}
        
        for case in feedback_cases:
            feedback_count = case["feedback_count"]
            description = case["description"]
            
            print(f"  ğŸ’­ Testing {description} ({feedback_count} feedback items)")
            
            # é †æ¬¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†
            sequential_time = await self._benchmark_sequential_hitl(feedback_count)
            
            # ä¸¦åˆ—ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†
            parallel_time = await self._benchmark_parallel_hitl(feedback_count)
            
            improvement = (sequential_time - parallel_time) / sequential_time
            
            hitl_results[f"{feedback_count}_feedback"] = {
                "description": description,
                "feedback_count": feedback_count,
                "sequential_time": sequential_time,
                "parallel_time": parallel_time,
                "improvement_ratio": improvement,
                "throughput": feedback_count / parallel_time if parallel_time > 0 else 0
            }
            
            print(f"     Sequential: {sequential_time:.2f}s")
            print(f"     Parallel: {parallel_time:.2f}s")
            print(f"     Improvement: {improvement:.1%}")
        
        return hitl_results
    
    async def _benchmark_sequential_hitl(self, feedback_count: int) -> float:
        """é †æ¬¡HITLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        start_time = time.time()
        
        for i in range(feedback_count):
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†æ™‚é–“ï¼ˆ0.3ç§’å¹³å‡ï¼‰
            await asyncio.sleep(0.003)  # é«˜é€ŸåŒ–å®Ÿè¡Œ
        
        return (time.time() - start_time) * 100
    
    async def _benchmark_parallel_hitl(self, feedback_count: int) -> float:
        """ä¸¦åˆ—HITLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        start_time = time.time()
        
        semaphore = asyncio.Semaphore(5)
        
        async def process_feedback_parallel(feedback_id: int):
            async with semaphore:
                # ä¸¦åˆ—ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†æ™‚é–“ï¼ˆ0.25ç§’å¹³å‡ï¼‰
                await asyncio.sleep(0.0025)
                return feedback_id
        
        tasks = [process_feedback_parallel(i) for i in range(feedback_count)]
        await asyncio.gather(*tasks)
        
        return (time.time() - start_time) * 100
    
    async def run_system_integration_benchmark(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("ğŸ”„ Running System Integration Benchmark...")
        
        # çµ±åˆã‚·ãƒŠãƒªã‚ª
        integration_scenarios = [
            {"sessions": 1, "phases": 7, "description": "Single session full pipeline"},
            {"sessions": 3, "phases": 7, "description": "Multi-session concurrent"},
            {"sessions": 5, "phases": 7, "description": "High load concurrent"}
        ]
        
        integration_results = {}
        
        for scenario in integration_scenarios:
            session_count = scenario["sessions"]
            phase_count = scenario["phases"]
            description = scenario["description"]
            
            print(f"  ğŸ­ Testing {description}")
            
            # ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            start_time = time.time()
            
            # è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸¦è¡Œå®Ÿè¡Œ
            session_tasks = []
            for session_id in range(session_count):
                task = self._simulate_full_pipeline(f"session_{session_id}")
                session_tasks.append(task)
            
            results = await asyncio.gather(*session_tasks)
            
            total_time = time.time() - start_time
            throughput = session_count / total_time if total_time > 0 else 0
            average_session_time = sum(r["execution_time"] for r in results) / len(results)
            
            integration_results[f"{session_count}_sessions"] = {
                "description": description,
                "session_count": session_count,
                "total_time": total_time,
                "average_session_time": average_session_time,
                "throughput": throughput,
                "success_rate": sum(1 for r in results if r["success"]) / len(results)
            }
            
            print(f"     Total Time: {total_time:.2f}s")
            print(f"     Throughput: {throughput:.2f} sessions/sec")
            print(f"     Avg Session Time: {average_session_time:.2f}s")
        
        return integration_results
    
    async def _simulate_full_pipeline(self, session_id: str) -> Dict[str, Any]:
        """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        start_time = time.time()
        
        # 7ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ
        for phase in range(1, 8):
            if phase == 5:
                # ãƒ•ã‚§ãƒ¼ã‚º5ã¯ä¸¦åˆ—ç”»åƒç”Ÿæˆ
                await self._benchmark_parallel_processing(12)  # 12ã‚·ãƒ¼ãƒ³
            else:
                # ãã®ä»–ãƒ•ã‚§ãƒ¼ã‚º
                phase_time = 0.01 * (phase + 1)  # ãƒ•ã‚§ãƒ¼ã‚ºã«å¿œã˜ãŸå‡¦ç†æ™‚é–“
                await asyncio.sleep(phase_time)
        
        execution_time = time.time() - start_time
        
        return {
            "session_id": session_id,
            "execution_time": execution_time,
            "success": True
        }
    
    async def run_all_benchmarks(self) -> Dict[str, Any]:
        """å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸ¯ Starting Comprehensive Performance Benchmark")
        print("=" * 60)
        
        # å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        self.results["phase_benchmarks"]["phase5"] = await self.run_phase5_benchmark()
        self.results["phase_benchmarks"]["quality_gates"] = await self.run_quality_gates_benchmark()
        self.results["phase_benchmarks"]["hitl_feedback"] = await self.run_hitl_feedback_benchmark()
        self.results["system_benchmarks"]["integration"] = await self.run_system_integration_benchmark()
        
        # ç·åˆåˆ†æ
        self._analyze_overall_performance()
        
        print("\n" + "=" * 60)
        print("âœ… Benchmark Complete!")
        
        return self.results
    
    def _analyze_overall_performance(self):
        """ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        print("\nğŸ“ˆ Overall Performance Analysis:")
        
        # ãƒ•ã‚§ãƒ¼ã‚º5æ”¹å–„åŠ¹æœ
        phase5_results = self.results["phase_benchmarks"]["phase5"]
        avg_phase5_improvement = sum(
            result["improvement_ratio"] 
            for result in phase5_results.values()
        ) / len(phase5_results)
        
        # å“è³ªã‚²ãƒ¼ãƒˆæ”¹å–„åŠ¹æœ
        quality_results = self.results["phase_benchmarks"]["quality_gates"]
        avg_quality_improvement = sum(
            result["improvement_ratio"]
            for result in quality_results.values()
        ) / len(quality_results)
        
        # HITLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ”¹å–„åŠ¹æœ
        hitl_results = self.results["phase_benchmarks"]["hitl_feedback"]
        avg_hitl_improvement = sum(
            result["improvement_ratio"]
            for result in hitl_results.values()
        ) / len(hitl_results)
        
        print(f"   Phase 5 Average Improvement: {avg_phase5_improvement:.1%}")
        print(f"   Quality Gates Average Improvement: {avg_quality_improvement:.1%}")
        print(f"   HITL Feedback Average Improvement: {avg_hitl_improvement:.1%}")
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = (avg_phase5_improvement + avg_quality_improvement + avg_hitl_improvement) / 3
        print(f"   Overall Performance Score: {overall_score:.1%}")
        
        # çµæœä¿å­˜
        self.results["comparison_results"] = {
            "phase5_improvement": avg_phase5_improvement,
            "quality_improvement": avg_quality_improvement,
            "hitl_improvement": avg_hitl_improvement,
            "overall_score": overall_score
        }
    
    def save_results(self, output_file: str = None):
        """çµæœä¿å­˜"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"performance_benchmark_results_{timestamp}.json"
        
        output_path = project_root / "backend" / "docs" / output_file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {output_path}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="Performance Benchmark Script")
    parser.add_argument("--output", "-o", help="Output file path for results")
    parser.add_argument("--quick", "-q", action="store_true", help="Run quick benchmark")
    
    args = parser.parse_args()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark = PerformanceBenchmark()
    
    if args.quick:
        print("ğŸš€ Running Quick Benchmark")
        results = await benchmark.run_phase5_benchmark()
    else:
        results = await benchmark.run_all_benchmarks()
    
    # çµæœä¿å­˜
    benchmark.save_results(args.output)
    
    return results


if __name__ == "__main__":
    asyncio.run(main())