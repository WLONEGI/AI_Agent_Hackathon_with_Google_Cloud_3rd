# AIæ¼«ç”»ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ ãƒ†ã‚¹ãƒˆè¨­è¨ˆæ›¸

**æ–‡æ›¸ç®¡ç†æƒ…å ±**
- æ–‡æ›¸ID: TEST-DOC-001
- ä½œæˆæ—¥: 2025-01-20
- ç‰ˆæ•°: 1.0
- æ‰¿èªè€…: æ ¹å²¸ç¥æ¨¹
- é–¢é€£æ–‡æ›¸: SYS-DOC-001ï¼ˆã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆæ›¸ï¼‰ã€SEC-DOC-001ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­è¨ˆæ›¸ï¼‰

## ç›®æ¬¡

- [1. ãƒ†ã‚¹ãƒˆæ¦‚è¦](#1-ãƒ†ã‚¹ãƒˆæ¦‚è¦)
  - [1.1 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥](#11-ãƒ†ã‚¹ãƒˆæˆ¦ç•¥)
  - [1.2 å“è³ªç›®æ¨™](#12-å“è³ªç›®æ¨™)
- [2. AIå“è³ªä¿è¨¼è¨­è¨ˆ](#2-aiå“è³ªä¿è¨¼è¨­è¨ˆ)
  - [2.1 7ãƒ•ã‚§ãƒ¼ã‚ºå“è³ªã‚²ãƒ¼ãƒˆ](#21-7ãƒ•ã‚§ãƒ¼ã‚ºå“è³ªã‚²ãƒ¼ãƒˆ)
  - [2.2 å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º](#22-å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º)
  - [2.3 è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹](#23-è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹)
- [3. è‡ªå‹•ãƒ†ã‚¹ãƒˆè¨­è¨ˆ](#3-è‡ªå‹•ãƒ†ã‚¹ãƒˆè¨­è¨ˆ)
  - [3.1 å˜ä½“ãƒ†ã‚¹ãƒˆ](#31-å˜ä½“ãƒ†ã‚¹ãƒˆ)
  - [3.2 çµ±åˆãƒ†ã‚¹ãƒˆ](#32-çµ±åˆãƒ†ã‚¹ãƒˆ)
  - [3.3 E2Eãƒ†ã‚¹ãƒˆ](#33-e2eãƒ†ã‚¹ãƒˆ)
- [4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆè¨­è¨ˆ](#4-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆè¨­è¨ˆ)
  - [4.1 è² è·ãƒ†ã‚¹ãƒˆ](#41-è² è·ãƒ†ã‚¹ãƒˆ)
  - [4.2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š](#42-ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š)
  - [4.3 ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š](#43-ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š)
- [5. AIå‡ºåŠ›å“è³ªãƒ†ã‚¹ãƒˆè¨­è¨ˆ](#5-aiå‡ºåŠ›å“è³ªãƒ†ã‚¹ãƒˆè¨­è¨ˆ)
  - [5.1 ç”»åƒç”Ÿæˆå“è³ª](#51-ç”»åƒç”Ÿæˆå“è³ª)
  - [5.2 ãƒ†ã‚­ã‚¹ãƒˆé…ç½®ç²¾åº¦](#52-ãƒ†ã‚­ã‚¹ãƒˆé…ç½®ç²¾åº¦)
  - [5.3 è¦–è¦šå“è³ªã‚¹ã‚³ã‚¢](#53-è¦–è¦šå“è³ªã‚¹ã‚³ã‚¢)
- [6. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†](#6-ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†)
  - [6.1 AIãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ](#61-aiãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ)
  - [6.2 å“è³ªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—](#62-å“è³ªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—)
  - [6.3 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ›´æ–°æˆ¦ç•¥](#63-ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ›´æ–°æˆ¦ç•¥)
- [7. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ](#7-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ)
  - [7.1 è‘—ä½œæ¨©ä¿è­·ãƒ†ã‚¹ãƒˆ](#71-è‘—ä½œæ¨©ä¿è­·ãƒ†ã‚¹ãƒˆ)
  - [7.2 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ†ã‚¹ãƒˆ](#72-ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ†ã‚¹ãƒˆ)
  - [7.3 èªè¨¼ãƒ»èªå¯ãƒ†ã‚¹ãƒˆ](#73-èªè¨¼èªå¯ãƒ†ã‚¹ãƒˆ)
- [8. CI/CDçµ±åˆ](#8-cicdçµ±åˆ)
  - [8.1 ãƒ†ã‚¹ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](#81-ãƒ†ã‚¹ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
  - [8.2 å“è³ªã‚²ãƒ¼ãƒˆè¨­å®š](#82-å“è³ªã‚²ãƒ¼ãƒˆè¨­å®š)
  - [8.3 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰¿èª](#83-ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰¿èª)
- [9. ãƒ†ã‚¹ãƒˆç’°å¢ƒç®¡ç†](#9-ãƒ†ã‚¹ãƒˆç’°å¢ƒç®¡ç†)

---

## 1. ãƒ†ã‚¹ãƒˆæ¦‚è¦

### 1.1 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

#### åŸºæœ¬æ–¹é‡
| é …ç›® | æ–¹é‡ | å®Ÿè£…ãƒ¬ãƒ™ãƒ« |
|------|------|----------|
| å“è³ªå„ªå…ˆ | 85%å“è³ªã‚¹ã‚³ã‚¢é”æˆã‚’æœ€å„ªå…ˆ | åŸºæœ¬ |
| è‡ªå‹•åŒ–é‡è¦– | åå¾©å¯èƒ½ãªãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ– | åŸºæœ¬ |
| åŠ¹ç‡æ€§è¿½æ±‚ | æœ€å°é™ã®ãƒ†ã‚¹ãƒˆå·¥æ•°ã§æœ€å¤§åŠ¹æœ | åŸºæœ¬ |
| ç¶™ç¶šæ”¹å–„ | AIã«ã‚ˆã‚‹ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹é€²åŒ– | é«˜åº¦ |

#### ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰
```mermaid
graph TD
    subgraph "ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰"
        A[å˜ä½“ãƒ†ã‚¹ãƒˆ - 70%]
        B[çµ±åˆãƒ†ã‚¹ãƒˆ - 20%]  
        C[E2Eãƒ†ã‚¹ãƒˆ - 10%]
    end
    
    subgraph "AIå“è³ªãƒ†ã‚¹ãƒˆ"
        D[ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥å“è³ªãƒ†ã‚¹ãƒˆ]
        E[ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å“è³ªãƒ†ã‚¹ãƒˆ]
        F[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–]
    end
    
    A --> D
    B --> E  
    C --> F
```

### 1.2 å“è³ªç›®æ¨™

#### å“è³ªKPI
```yaml
Quality Targets:
  Functional Quality:
    Unit Test Coverage: 80%
    Integration Test Coverage: 70%
    E2E Test Coverage: ä¸»è¦ãƒ•ãƒ­ãƒ¼100%
    
  AI Quality:
    Phase Success Rate: 85% per phase
    End-to-End Success Rate: 70%
    User Satisfaction Score: 4.0/5.0
    
  Performance Quality:
    Response Time: < 100ms (API)
    Generation Time: < 10åˆ† (standard text)
    Concurrent Users: 100 users supported
    
  Security Quality:
    Vulnerability Scan: 0 Critical issues
    Copyright Detection: 95% accuracy
    Content Filter: 95% accuracy
```

---

## 2. AIå“è³ªä¿è¨¼è¨­è¨ˆ

### 2.1 7ãƒ•ã‚§ãƒ¼ã‚ºå“è³ªã‚²ãƒ¼ãƒˆ

#### ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥å“è³ªãƒã‚§ãƒƒã‚¯
```python
class AIQualityGate:
    def __init__(self):
        self.phase_validators = {
            1: TextAnalysisValidator(),
            2: StoryStructureValidator(), 
            3: SceneDivisionValidator(),
            4: CharacterDesignValidator(),
            5: PanelLayoutValidator(),
            6: ImageGenerationValidator(),
            7: DialogPlacementValidator(),
            8: FinalIntegrationValidator()
        }
        self.quality_threshold = 0.85  # 85%
        
    async def validate_phase_output(self, phase: int, input_data: dict, output_data: dict) -> dict:
        """
        å„ãƒ•ã‚§ãƒ¼ã‚ºã®å‡ºåŠ›å“è³ªæ¤œè¨¼
        """
        validator = self.phase_validators[phase]
        
        # ãƒ•ã‚§ãƒ¼ã‚ºå›ºæœ‰ã®å“è³ªãƒã‚§ãƒƒã‚¯
        quality_score = await validator.calculate_quality_score(input_data, output_data)
        
        # å…±é€šå“è³ªãƒã‚§ãƒƒã‚¯
        common_checks = await self.run_common_quality_checks(output_data)
        
        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
        total_score = (quality_score * 0.8) + (common_checks['score'] * 0.2)
        
        result = {
            'phase': phase,
            'quality_score': total_score,
            'passes_gate': total_score >= self.quality_threshold,
            'details': {
                'phase_specific_score': quality_score,
                'common_checks': common_checks,
                'recommendations': validator.get_improvement_suggestions(output_data)
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # å“è³ªãƒ­ã‚°è¨˜éŒ²
        await self.log_quality_result(result)
        
        return result

# ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ†ã‚­ã‚¹ãƒˆè§£æã®å“è³ªæ¤œè¨¼
class TextAnalysisValidator:
    async def calculate_quality_score(self, input_text: str, analysis_result: dict) -> float:
        scores = []
        
        # 1. æ–‡ç« æ§‹é€ ç†è§£åº¦
        structure_score = self.evaluate_structure_understanding(input_text, analysis_result)
        scores.append(structure_score)
        
        # 2. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡ºç²¾åº¦
        character_score = self.evaluate_character_extraction(input_text, analysis_result)
        scores.append(character_score)
        
        # 3. ãƒ†ãƒ¼ãƒç†è§£åº¦
        theme_score = self.evaluate_theme_understanding(input_text, analysis_result)
        scores.append(theme_score)
        
        return sum(scores) / len(scores)
    
    def evaluate_structure_understanding(self, input_text: str, result: dict) -> float:
        """
        æ–‡ç« æ§‹é€ ã®ç†è§£åº¦è©•ä¾¡
        """
        expected_chapters = self.count_expected_chapters(input_text)
        detected_chapters = len(result.get('chapters', []))
        
        # ç« æ•°ã®ä¸€è‡´åº¦
        chapter_accuracy = min(1.0, detected_chapters / max(1, expected_chapters))
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºç²¾åº¦
        keyword_accuracy = self.evaluate_keyword_extraction(input_text, result)
        
        return (chapter_accuracy * 0.6) + (keyword_accuracy * 0.4)

# ãƒ•ã‚§ãƒ¼ã‚º6: ç”»åƒç”Ÿæˆã®å“è³ªæ¤œè¨¼  
class ImageGenerationValidator:
    async def calculate_quality_score(self, scene_data: dict, generated_images: list) -> float:
        scores = []
        
        for image in generated_images:
            # 1. ç”»åƒå“è³ªï¼ˆæŠ€è¡“çš„ï¼‰
            technical_score = await self.evaluate_technical_quality(image['url'])
            
            # 2. ã‚·ãƒ¼ãƒ³ä¸€è‡´åº¦
            scene_match_score = await self.evaluate_scene_matching(scene_data, image)
            
            # 3. ã‚¹ã‚¿ã‚¤ãƒ«ä¸€è²«æ€§
            style_score = await self.evaluate_style_consistency(image)
            
            image_score = (technical_score * 0.4) + (scene_match_score * 0.4) + (style_score * 0.2)
            scores.append(image_score)
        
        return sum(scores) / len(scores) if scores else 0.0
```

### 2.2 å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º

#### å“è³ªã‚¹ã‚³ã‚¢å®šç¾©
```yaml
Quality Score Calculation:
  Phase 1 (Text Analysis):
    - Structure Understanding: 40%
    - Character Extraction: 30%  
    - Theme Detection: 30%
    Target: 85%
    
  Phase 2 (Story Structure):
    - Plot Coherence: 50%
    - Pacing Quality: 30%
    - Dramatic Arc: 20%
    Target: 85%
    
  Phase 3 (Scene Division):
    - Scene Boundary Accuracy: 60%
    - Transition Quality: 40%
    Target: 85%
    
  Phase 4 (Character Design):
    - Visual Consistency: 50%
    - Character Distinctiveness: 30%
    - Style Adherence: 20%
    Target: 85%
    
  Phase 5 (Panel Layout):
    - Layout Balance: 40%
    - Reading Flow: 40%
    - Space Efficiency: 20%
    Target: 85%
    
  Phase 6 (Image Generation):
    - Technical Quality: 40%
    - Scene Matching: 40%
    - Style Consistency: 20%
    Target: 85%
    
  Phase 7 (Dialog Placement):
    - Text Readability: 50%
    - Bubble Placement: 30%
    - Font Selection: 20%
    Target: 85%
    
  Phase 8 (Final Integration):
    - Overall Coherence: 60%
    - Technical Quality: 25%
    - User Experience: 15%
    Target: 85%
```

### 2.3 è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹

#### ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯
```python
class QualityGateManager:
    def __init__(self):
        self.max_retries = 3
        self.quality_threshold = 0.85
        
    async def execute_phase_with_quality_gate(self, phase: int, input_data: dict) -> dict:
        """
        å“è³ªã‚²ãƒ¼ãƒˆä»˜ããƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ
        """
        retry_count = 0
        best_result = None
        best_score = 0.0
        
        while retry_count < self.max_retries:
            try:
                # ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ
                output = await self.execute_phase(phase, input_data)
                
                # å“è³ªæ¤œè¨¼
                quality_result = await self.validate_phase_output(phase, input_data, output)
                
                # å“è³ªã‚¹ã‚³ã‚¢ãƒã‚§ãƒƒã‚¯
                if quality_result['quality_score'] >= self.quality_threshold:
                    # åˆæ ¼ - æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã¸
                    await self.log_quality_success(phase, quality_result)
                    return output
                
                # å“è³ªä¸è¶³ - æœ€é«˜ã‚¹ã‚³ã‚¢è¨˜éŒ²
                if quality_result['quality_score'] > best_score:
                    best_result = output
                    best_score = quality_result['quality_score']
                
                retry_count += 1
                await self.log_quality_retry(phase, retry_count, quality_result)
                
            except Exception as e:
                retry_count += 1
                await self.log_phase_error(phase, retry_count, str(e))
        
        # 3å›å¤±æ•—å¾Œã®å‡¦ç†
        if best_result and best_score > 0.75:  # 75%ä»¥ä¸Šãªã‚‰è¨±å®¹
            await self.log_quality_degraded_accept(phase, best_score)
            return best_result
        else:
            # å®Œå…¨å¤±æ•—
            raise QualityGateFailure(f"Phase {phase} failed to meet quality standards after {self.max_retries} attempts")
```

---

## 3. è‡ªå‹•ãƒ†ã‚¹ãƒˆè¨­è¨ˆ

### 3.1 å˜ä½“ãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™
```python
# pytestè¨­å®šä¾‹
import pytest
from unittest.mock import Mock, patch
import asyncio

class TestTextAnalysisService:
    """
    Phase 1: ãƒ†ã‚­ã‚¹ãƒˆè§£æã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ†ã‚¹ãƒˆ
    """
    
    def setup_method(self):
        self.service = TextAnalysisService()
        self.mock_gemini_client = Mock()
        
    @pytest.mark.asyncio
    async def test_basic_phase1_concept(self):
        """
        åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆè§£æãƒ†ã‚¹ãƒˆ
        """
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        input_text = "ä¸»äººå…¬ã¯é«˜æ ¡ç”Ÿã®å‰£å£«ã€‚é­”ç‹ã‚’å€’ã™ãŸã‚ä»²é–“ã¨å…±ã«å†’é™ºã«å‡ºã‚‹ã€‚"
        
        # æœŸå¾…çµæœ
        expected_result = {
            'characters': ['ä¸»äººå…¬', 'é­”ç‹', 'ä»²é–“'],
            'themes': ['å†’é™º', 'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼', 'æˆé•·'],
            'emotions': ['å‹‡æ°—', 'å‹æƒ…', 'å¸Œæœ›']
        }
        
        # å®Ÿè¡Œ
        with patch.object(self.service, 'gemini_client', self.mock_gemini_client):
            result = await self.service.analyze_text(input_text)
            
        # æ¤œè¨¼
        assert 'characters' in result
        assert len(result['characters']) >= 2
        assert 'themes' in result
        assert result['processing_time'] < 30  # 30ç§’ä»¥å†…
    
    @pytest.mark.asyncio
    async def test_long_phase1_concept(self):
        """
        é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆè§£æãƒ†ã‚¹ãƒˆï¼ˆ50,000æ–‡å­—ï¼‰
        """
        long_text = "é•·ç·¨å°èª¬ã®ãƒ†ã‚­ã‚¹ãƒˆ..." * 1000  # 50KBç›¸å½“
        
        result = await self.service.analyze_text(long_text)
        
        assert result['processing_time'] < 60  # 60ç§’ä»¥å†…
        assert len(result['characters']) <= 20  # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ•°åˆ¶é™
        
    @pytest.mark.asyncio  
    async def test_error_handling(self):
        """
        ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        """
        # ä¸æ­£ãªå…¥åŠ›
        with pytest.raises(ValidationError):
            await self.service.analyze_text("")
        
        # API ã‚¨ãƒ©ãƒ¼
        self.mock_gemini_client.side_effect = Exception("API Error")
        with pytest.raises(ServiceError):
            await self.service.analyze_text("ãƒ†ã‚¹ãƒˆ")

# å…¨ãƒ•ã‚§ãƒ¼ã‚ºå…±é€šãƒ†ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹
class BasePhaseTest:
    """
    å…¨ãƒ•ã‚§ãƒ¼ã‚ºå…±é€šã®ãƒ†ã‚¹ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹
    """
    
    def test_processing_time_limit(self):
        """å„ãƒ•ã‚§ãƒ¼ã‚ºã®å‡¦ç†æ™‚é–“åˆ¶é™ãƒ†ã‚¹ãƒˆ"""
        pass
        
    def test_input_validation(self):
        """å…¥åŠ›å€¤æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        pass
        
    def test_output_format(self):
        """å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        pass
        
    def test_error_recovery(self):
        """ã‚¨ãƒ©ãƒ¼å›å¾©ãƒ†ã‚¹ãƒˆ"""
        pass
```

### 3.2 çµ±åˆãƒ†ã‚¹ãƒˆ

#### ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“é€£æºãƒ†ã‚¹ãƒˆ
```python
class TestServiceIntegration:
    """
    ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“ã®çµ±åˆãƒ†ã‚¹ãƒˆ
    """
    
    @pytest.mark.asyncio
    async def test_phase_to_phase_flow(self):
        """
        ãƒ•ã‚§ãƒ¼ã‚ºé–“ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ ãƒ†ã‚¹ãƒˆ
        """
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        test_request = {
            'request_id': 'test-12345',
            'text': self.get_test_text(),
            'settings': {'style': 'å°‘å¹´æ¼«ç”»', 'pages': 20}
        }
        
        # Phase 1 å®Ÿè¡Œ
        phase1_concept_result = await self.execute_phase1_concept(test_request)
        assert phase1_concept_result['status'] == 'success'
        
        # Phase 2 å®Ÿè¡Œï¼ˆPhase 1ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨ï¼‰
        phase2_character_result = await self.execute_phase2_character(phase1_concept_result['output'])
        assert phase2_character_result['status'] == 'success'
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        self.verify_data_consistency(phase1_concept_result, phase2_character_result)
    
    @pytest.mark.asyncio
    async def test_pubsub_messaging(self):
        """
        Pub/Sub ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ³ã‚° ãƒ†ã‚¹ãƒˆ
        """
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
        test_message = {
            'request_id': 'test-67890',
            'phase': 1,
            'status': 'success',
            'data': {'redis_key': 'test:67890:phase1'}
        }
        
        await self.publish_message('phase1-completed', test_message)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡ç¢ºèª
        received_message = await self.wait_for_message('phase1-completed', timeout=10)
        assert received_message['request_id'] == test_message['request_id']
    
    @pytest.mark.asyncio
    async def test_redis_data_flow(self):
        """
        Redis ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ ãƒ†ã‚¹ãƒˆ  
        """
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
        test_data = {'test': 'data'}
        await self.redis_client.set('test:request:phase1', json.dumps(test_data))
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ¤œè¨¼
        retrieved_data = await self.redis_client.get('test:request:phase1')
        assert json.loads(retrieved_data) == test_data
        
        # TTLç¢ºèª
        ttl = await self.redis_client.ttl('test:request:phase1')
        assert ttl > 0  # TTLãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨
```

### 3.3 E2Eãƒ†ã‚¹ãƒˆ

#### HITLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
```python
class TestHITLFeedback:
    """
    Human-in-the-Loop ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
    """
    
    @pytest.mark.asyncio
    async def test_phase_preview_generation(self):
        """
        å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        """
        for phase_num in range(1, 8):
            preview_data = await self.generate_phase_preview(phase_num)
            assert preview_data['status'] == 'success'
            assert 'preview_content' in preview_data
            assert preview_data['phase'] == phase_num
    
    @pytest.mark.asyncio  
    async def test_chat_feedback_processing(self):
        """
        ãƒãƒ£ãƒƒãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‡¦ç†ãƒ†ã‚¹ãƒˆ
        """
        # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
        feedback_message = {
            'request_id': 'test-123',
            'phase': 2,
            'message': 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’ã‚‚ã£ã¨æ˜ã‚‹ã„æ€§æ ¼ã«ã—ã¦ãã ã•ã„',
            'message_type': 'text'
        }
        
        response = await self.send_chat_feedback(feedback_message)
        assert response['status'] == 'processed'
        assert 'parsed_intent' in response
        assert response['parsed_intent']['action'] == 'modify_character'
    
    @pytest.mark.asyncio
    async def test_feedback_timeout_handling(self):
        """
        ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆï¼ˆ30åˆ†ï¼‰
        """
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        start_time = time.time()
        result = await self.wait_for_feedback_with_timeout(
            request_id='test-456',
            phase=3,
            timeout_seconds=1  # ãƒ†ã‚¹ãƒˆç”¨ã«1ç§’ã«çŸ­ç¸®
        )
        
        assert result['status'] == 'timeout'
        assert time.time() - start_time >= 1
    
    @pytest.mark.asyncio
    async def test_preview_version_branching(self):
        """
        ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³åˆ†å²ãƒ†ã‚¹ãƒˆ
        """
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä½œæˆ
        original = await self.create_preview_version('test-789', 1)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é©ç”¨ã§åˆ†å²
        branch1 = await self.apply_feedback_and_branch(
            original['version_id'],
            'ãƒ†ãƒ¼ãƒã‚’å¤‰æ›´'
        )
        
        # åˆ¥ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã§åˆ¥åˆ†å²
        branch2 = await self.apply_feedback_and_branch(
            original['version_id'],
            'ã‚¸ãƒ£ãƒ³ãƒ«ã‚’å¤‰æ›´'
        )
        
        assert branch1['parent_version_id'] == original['version_id']
        assert branch2['parent_version_id'] == original['version_id']
        assert branch1['version_id'] != branch2['version_id']
    
    @pytest.mark.asyncio
    async def test_websocket_realtime_updates(self):
        """
        WebSocketãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ãƒ†ã‚¹ãƒˆ
        """
        async with websockets.connect('ws://localhost:8000/ws') as websocket:
            # ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
            await websocket.send(json.dumps({
                'type': 'phase_complete',
                'phase': 1,
                'request_id': 'test-ws-123'
            }))
            
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ›´æ–°å—ä¿¡
            response = await websocket.recv()
            data = json.loads(response)
            assert data['type'] == 'preview_ready'
            assert data['phase'] == 1
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡
            await websocket.send(json.dumps({
                'type': 'feedback',
                'phase': 1,
                'feedback': 'ã‚‚ã£ã¨æ˜ã‚‹ã„ãƒ†ãƒ¼ãƒã«'
            }))
            
            # å‡¦ç†çµæœå—ä¿¡
            response = await websocket.recv()
            data = json.loads(response)
            assert data['type'] == 'feedback_applied'
```

#### ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
```python
import playwright
from playwright.async_api import async_playwright

class TestEndToEndFlow:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦–ç‚¹ã§ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
    """
    
    @pytest.mark.asyncio
    async def test_complete_manga_generation_flow(self):
        """
        å®Œå…¨ãªæ¼«ç”»ç”Ÿæˆãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()
            
            # 1. ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
            await page.goto('https://manga-service.com')
            await page.wait_for_load_state('networkidle')
            
            # 2. ãƒ­ã‚°ã‚¤ãƒ³
            await page.click('[data-testid="login-button"]')
            await page.fill('[data-testid="email-input"]', 'test@example.com')
            await page.fill('[data-testid="password-input"]', 'testpassword')
            await page.click('[data-testid="login-submit"]')
            
            # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›
            test_prompt = "ä¸»äººå…¬ã¯é«˜æ ¡ç”Ÿã®å‰£å£«ã€‚é­”ç‹ã‚’å€’ã™ãŸã‚ä»²é–“ã¨å…±ã«å†’é™ºã«å‡ºã‚‹ã€‚"
            await page.fill('[data-testid="prompt-input"]', test_prompt)
            
            # 4. ã‚¹ã‚¿ã‚¤ãƒ«é¸æŠ
            await page.click('[data-testid="style-å°‘å¹´æ¼«ç”»"]')
            
            # 5. ç”Ÿæˆé–‹å§‹
            await page.click('[data-testid="generate-button"]')
            
            # 6. é€²æ—ç¢ºèª
            await page.wait_for_selector('[data-testid="progress-phase1"]', timeout=30000)
            
            # 7. å®Œäº†ã¾ã§å¾…æ©Ÿï¼ˆæœ€å¤§12åˆ†ï¼‰
            await page.wait_for_selector('[data-testid="generation-complete"]', timeout=720000)
            
            # 8. çµæœç¢ºèª
            manga_pages = await page.query_selector_all('[data-testid="manga-page"]')
            assert len(manga_pages) >= 10  # æœ€å°ãƒšãƒ¼ã‚¸æ•°ç¢ºèª
            
            # 9. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèª
            async with page.expect_download() as download_info:
                await page.click('[data-testid="download-button"]')
            download = await download_info.value
            assert download.suggested_filename.endswith('.pdf')
            
            await browser.close()
```

---

## 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆè¨­è¨ˆ

### 4.1 è² è·ãƒ†ã‚¹ãƒˆ

#### 100åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼è² è·ãƒ†ã‚¹ãƒˆ
```python
import locust
from locust import HttpUser, task, between

class MangaGenerationUser(HttpUser):
    """
    æ¼«ç”»ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹ã®è² è·ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼
    """
    wait_time = between(1, 3)  # 1-3ç§’ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œé–“éš”
    
    def on_start(self):
        """ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼åˆæœŸåŒ–"""
        self.login()
        
    def login(self):
        """ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†"""
        response = self.client.post("/api/auth/login", json={
            "email": f"test{self.user_id}@example.com",
            "password": "testpassword"
        })
        
        if response.status_code == 200:
            self.auth_token = response.json()['token']
            self.client.headers.update({
                'Authorization': f'Bearer {self.auth_token}'
            })
    
    @task(3)
    def browse_gallery(self):
        """ã‚®ãƒ£ãƒ©ãƒªãƒ¼é–²è¦§ï¼ˆè»½ã„å‡¦ç†ï¼‰"""
        self.client.get("/api/manga/gallery")
    
    @task(1)  
    def generate_manga(self):
        """æ¼«ç”»ç”Ÿæˆï¼ˆé‡ã„å‡¦ç†ï¼‰"""
        test_prompts = [
            "é«˜æ ¡ç”Ÿã®ä¸»äººå…¬ãŒé­”æ³•å­¦æ ¡ã§å†’é™ºã™ã‚‹ç‰©èª",
            "æœªæ¥ã®å®‡å®™ã§æˆ¦ã†ãƒ­ãƒœãƒƒãƒˆæ¼«ç”»",
            "æ—¥å¸¸ç³»ã‚³ãƒ¡ãƒ‡ã‚£æ¼«ç”»ã®ç‰©èª"
        ]
        
        response = self.client.post("/api/manga/generate", json={
            "text": self.random.choice(test_prompts),
            "style": "å°‘å¹´æ¼«ç”»",
            "pages": 20
        })
        
        if response.status_code == 202:  # éåŒæœŸå‡¦ç†é–‹å§‹
            request_id = response.json()['request_id']
            
            # å®Œäº†ã¾ã§å¾…æ©Ÿï¼ˆãƒãƒ¼ãƒªãƒ³ã‚°ï¼‰
            self.wait_for_completion(request_id)
    
    def wait_for_completion(self, request_id: str):
        """ç”Ÿæˆå®Œäº†ã¾ã§å¾…æ©Ÿ"""
        import time
        start_time = time.time()
        
        while time.time() - start_time < 720:  # 12åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            response = self.client.get(f"/api/manga/status/{request_id}")
            
            if response.status_code == 200:
                status = response.json()['status']
                if status == 'completed':
                    break
                elif status == 'failed':
                    break
            
            time.sleep(30)  # 30ç§’é–“éš”ã§ãƒãƒ¼ãƒªãƒ³ã‚°

# è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œè¨­å®š
class LoadTestConfig:
    users = 100           # åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
    spawn_rate = 10       # 10ãƒ¦ãƒ¼ã‚¶ãƒ¼/ç§’ã§å¢—åŠ 
    duration = "10m"      # 10åˆ†é–“å®Ÿè¡Œ
    
    performance_targets = {
        'api_response_time_95th': 2000,  # 95%ile < 2ç§’
        'manga_generation_time': 600,    # å¹³å‡10åˆ†ä»¥å†…
        'error_rate': 0.01,              # ã‚¨ãƒ©ãƒ¼ç‡1%ä»¥ä¸‹
        'throughput': 50                 # 50ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†
    }
```

### 4.2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        
    async def measure_api_performance(self, endpoint: str, payload: dict) -> dict:
        """
        API ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        """
        start_time = time.time()
        
        try:
            response = await self.make_api_request(endpoint, payload)
            end_time = time.time()
            
            metrics = {
                'endpoint': endpoint,
                'response_time': end_time - start_time,
                'status_code': response.status_code,
                'payload_size': len(json.dumps(payload)),
                'response_size': len(response.content),
                'timestamp': datetime.now().isoformat()
            }
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            await self.metrics_collector.record_api_metrics(metrics)
            
            return metrics
            
        except Exception as e:
            await self.metrics_collector.record_api_error(endpoint, str(e))
            raise

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
performance_test_cases = [
    {
        'name': 'login_performance',
        'endpoint': '/api/auth/login',
        'target_time': 0.5,  # 500msä»¥å†…
        'concurrent_users': 50
    },
    {
        'name': 'gallery_load_performance', 
        'endpoint': '/api/manga/gallery',
        'target_time': 1.0,  # 1ç§’ä»¥å†…
        'concurrent_users': 100
    },
    {
        'name': 'generation_start_performance',
        'endpoint': '/api/manga/generate',
        'target_time': 2.0,  # 2ç§’ä»¥å†…ï¼ˆéåŒæœŸé–‹å§‹ï¼‰
        'concurrent_users': 20
    }
]
```

### 4.3 ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
```python
class BottleneckAnalyzer:
    def __init__(self):
        self.profiler = ProfilerManager()
        
    async def analyze_phase_performance(self, phase: int) -> dict:
        """
        ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        """
        # CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
        resource_usage = await self.measure_resource_usage(phase)
        
        # APIå‘¼ã³å‡ºã—æ™‚é–“åˆ†æ
        api_timing = await self.analyze_api_timing(phase)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ ã‚¯ã‚¨ãƒªåˆ†æ
        db_performance = await self.analyze_database_queries(phase)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
        bottlenecks = self.identify_bottlenecks({
            'resource_usage': resource_usage,
            'api_timing': api_timing,
            'db_performance': db_performance
        })
        
        return {
            'phase': phase,
            'bottlenecks': bottlenecks,
            'recommendations': self.generate_optimization_recommendations(bottlenecks),
            'performance_score': self.calculate_performance_score(resource_usage, api_timing)
        }
    
    def identify_bottlenecks(self, metrics: dict) -> list:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
        """
        bottlenecks = []
        
        # CPUä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if metrics['resource_usage']['cpu_percent'] > 80:
            bottlenecks.append({
                'type': 'cpu_bottleneck',
                'severity': 'high',
                'value': metrics['resource_usage']['cpu_percent']
            })
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if metrics['resource_usage']['memory_percent'] > 85:
            bottlenecks.append({
                'type': 'memory_bottleneck',
                'severity': 'high', 
                'value': metrics['resource_usage']['memory_percent']
            })
        
        # APIå¿œç­”æ™‚é–“ãƒã‚§ãƒƒã‚¯
        if metrics['api_timing']['avg_response_time'] > 30:
            bottlenecks.append({
                'type': 'api_latency',
                'severity': 'medium',
                'value': metrics['api_timing']['avg_response_time']
            })
        
        return bottlenecks
```

---

## 5. AIå‡ºåŠ›å“è³ªãƒ†ã‚¹ãƒˆè¨­è¨ˆ

### 5.1 ç”»åƒç”Ÿæˆå“è³ª

#### ç”»åƒå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
```python
class ImageQualityTester:
    def __init__(self):
        self.quality_models = self.load_quality_models()
        
    async def evaluate_image_quality(self, image_url: str, scene_context: dict) -> dict:
        """
        ç”Ÿæˆç”»åƒã®å“è³ªè©•ä¾¡
        """
        # 1. æŠ€è¡“çš„å“è³ªè©•ä¾¡
        technical_score = await self.evaluate_technical_quality(image_url)
        
        # 2. ã‚·ãƒ¼ãƒ³ä¸€è‡´åº¦è©•ä¾¡
        scene_score = await self.evaluate_scene_matching(image_url, scene_context)
        
        # 3. ã‚¹ã‚¿ã‚¤ãƒ«ä¸€è²«æ€§è©•ä¾¡
        style_score = await self.evaluate_style_consistency(image_url)
        
        overall_score = (technical_score * 0.4) + (scene_score * 0.4) + (style_score * 0.2)
        
        return {
            'overall_score': overall_score,
            'technical_score': technical_score,
            'scene_score': scene_score,
            'style_score': style_score,
            'passes_quality_gate': overall_score >= 0.85,
            'recommendations': self.generate_improvement_suggestions(
                technical_score, scene_score, style_score
            )
        }
    
    async def evaluate_technical_quality(self, image_url: str) -> float:
        """
        æŠ€è¡“çš„ç”»åƒå“è³ªè©•ä¾¡
        """
        from PIL import Image
        import numpy as np
        
        # ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»è§£æ
        image = await self.download_image(image_url)
        
        scores = []
        
        # è§£åƒåº¦ãƒã‚§ãƒƒã‚¯
        width, height = image.size
        resolution_score = min(1.0, (width * height) / (1024 * 1024))  # 1MPåŸºæº–
        scores.append(resolution_score)
        
        # ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹è©•ä¾¡ï¼ˆãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³åˆ†æ•£ï¼‰
        gray_image = image.convert('L')
        laplacian_var = cv2.Laplacian(np.array(gray_image), cv2.CV_64F).var()
        sharpness_score = min(1.0, laplacian_var / 1000)  # é–¾å€¤èª¿æ•´
        scores.append(sharpness_score)
        
        # è‰²å½©ãƒãƒ©ãƒ³ã‚¹
        color_score = self.evaluate_color_balance(image)
        scores.append(color_score)
        
        return sum(scores) / len(scores)
```

### 5.2 ãƒ†ã‚­ã‚¹ãƒˆé…ç½®ç²¾åº¦

#### ã‚»ãƒªãƒ•é…ç½®å“è³ªãƒ†ã‚¹ãƒˆ
```python
class DialogPlacementTester:
    async def test_text_readability(self, manga_page: dict) -> dict:
        """
        ãƒ†ã‚­ã‚¹ãƒˆå¯èª­æ€§ãƒ†ã‚¹ãƒˆ
        """
        scores = []
        
        for dialog in manga_page['dialogs']:
            # 1. ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºé©åˆ‡æ€§
            font_score = self.evaluate_font_size(dialog)
            
            # 2. å¹ãå‡ºã—é…ç½®
            bubble_score = self.evaluate_bubble_placement(dialog, manga_page['layout'])
            
            # 3. èƒŒæ™¯ã¨ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ
            contrast_score = self.evaluate_text_contrast(dialog, manga_page['background'])
            
            dialog_score = (font_score + bubble_score + contrast_score) / 3
            scores.append(dialog_score)
        
        return {
            'average_readability': sum(scores) / len(scores) if scores else 0,
            'readability_distribution': scores,
            'passes_readability_test': all(score >= 0.8 for score in scores)
        }
    
    def evaluate_font_size(self, dialog: dict) -> float:
        """
        ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºé©åˆ‡æ€§è©•ä¾¡
        """
        font_size = dialog['font_size']
        text_length = len(dialog['text'])
        bubble_area = dialog['bubble_area']
        
        # é¢ç©ã‚ãŸã‚Šã®æ–‡å­—ã‚µã‚¤ã‚ºé©åˆ‡æ€§
        text_density = text_length / bubble_area
        optimal_font_size = self.calculate_optimal_font_size(text_density)
        
        size_difference = abs(font_size - optimal_font_size) / optimal_font_size
        return max(0, 1.0 - size_difference)
```

### 5.3 è¦–è¦šå“è³ªã‚¹ã‚³ã‚¢

#### ç·åˆè¦–è¦šå“è³ªè©•ä¾¡
```python
class VisualQualityScorer:
    def __init__(self):
        self.quality_weights = {
            'composition': 0.3,      # æ§‹å›³
            'color_harmony': 0.2,    # è‰²å½©èª¿å’Œ
            'detail_quality': 0.2,   # è©³ç´°åº¦
            'style_consistency': 0.3  # ã‚¹ã‚¿ã‚¤ãƒ«ä¸€è²«æ€§
        }
    
    async def calculate_manga_quality_score(self, manga_pages: list) -> dict:
        """
        æ¼«ç”»å…¨ä½“ã®è¦–è¦šå“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
        """
        page_scores = []
        
        for page in manga_pages:
            page_score = await self.evaluate_page_quality(page)
            page_scores.append(page_score)
        
        # å…¨ä½“ã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = sum(page_scores) / len(page_scores)
        
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
        consistency_score = self.calculate_consistency_across_pages(manga_pages)
        
        # æœ€çµ‚å“è³ªã‚¹ã‚³ã‚¢
        final_score = (overall_score * 0.8) + (consistency_score * 0.2)
        
        return {
            'overall_quality_score': final_score,
            'page_scores': page_scores,
            'consistency_score': consistency_score,
            'passes_quality_threshold': final_score >= 0.85,
            'quality_breakdown': self.analyze_quality_components(manga_pages)
        }
```

---

## 6. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†

### 6.1 AIãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ

#### è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
```python
class AITestCaseGenerator:
    def __init__(self):
        self.gemini_client = self.initialize_gemini_client()
        self.test_case_templates = self.load_templates()
        
    async def generate_test_scenarios(self, count: int = 50) -> list:
        """
        AIã«ã‚ˆã‚‹ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªè‡ªå‹•ç”Ÿæˆ
        """
        generated_scenarios = []
        
        for category in ['adventure', 'romance', 'comedy', 'action', 'slice_of_life']:
            category_scenarios = await self.generate_category_scenarios(category, count // 5)
            generated_scenarios.extend(category_scenarios)
        
        # å“è³ªæ¤œè¨¼
        validated_scenarios = await self.validate_generated_scenarios(generated_scenarios)
        
        return validated_scenarios
    
    async def generate_category_scenarios(self, category: str, count: int) -> list:
        """
        ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªç”Ÿæˆ
        """
        prompt = f"""
        æ¼«ç”»ã®{category}ã‚¸ãƒ£ãƒ³ãƒ«ã®ãƒ†ã‚¹ãƒˆç”¨ã‚ã‚‰ã™ã˜ã‚’{count}å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        
        è¦ä»¶:
        - å„ã‚ã‚‰ã™ã˜ã¯100-500æ–‡å­—
        - å¤šæ§˜ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
        - æ˜ç¢ºãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ§‹é€ 
        - ãƒ†ã‚¹ãƒˆç”¨é€”ã«é©ã—ãŸå†…å®¹
        
        å½¢å¼:
        1. [ã‚¿ã‚¤ãƒˆãƒ«]: [ã‚ã‚‰ã™ã˜]
        2. [ã‚¿ã‚¤ãƒˆãƒ«]: [ã‚ã‚‰ã™ã˜]
        ...
        """
        
        response = await self.gemini_client.generate_content(prompt)
        scenarios = self.parse_scenarios(response.text)
        
        return scenarios
    
    async def create_edge_case_tests(self) -> list:
        """
        ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆç”Ÿæˆ
        """
        edge_cases = [
            # æ¥µç«¯ã«çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆ
            {"text": "ä¸»äººå…¬ãŒæˆ¦ã†ã€‚", "expected_result": "error", "reason": "too_short"},
            
            # æ¥µç«¯ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ  
            {"text": "é•·ç·¨å°èª¬..." * 10000, "expected_result": "error", "reason": "too_long"},
            
            # æ–‡å­—æ•°å¢ƒç•Œå€¤
            {"text": "a" * 49999, "expected_result": "success", "reason": "boundary_test"},
            {"text": "a" * 50001, "expected_result": "error", "reason": "boundary_test"},
            
            # ç‰¹æ®Šæ–‡å­—
            {"text": "ä¸»äººå…¬ã¯ğŸ—¾ã§ğŸ¯ã‚’âš”ï¸ã§å®ˆã‚‹", "expected_result": "success", "reason": "emoji_test"},
            
            # è‘—ä½œæ¨©å¢ƒç•Œã‚±ãƒ¼ã‚¹
            {"text": "ä¸»äººå…¬ã®åå‰ã¯ãƒŠãƒ«ãƒˆã§ã™", "expected_result": "error", "reason": "copyright"},
            {"text": "å¿è€…ã®ç‰©èª", "expected_result": "success", "reason": "generic_term"}
        ]
        
        return edge_cases
```

### 6.2 å“è³ªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—

#### ç¶™ç¶šçš„å“è³ªæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 
```python
class QualityFeedbackLoop:
    def __init__(self):
        self.quality_history = QualityHistoryManager()
        self.improvement_engine = QualityImprovementEngine()
        
    async def collect_user_feedback(self, manga_id: str, user_rating: dict):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã®åé›†
        """
        feedback_data = {
            'manga_id': manga_id,
            'user_id': user_rating['user_id'],
            'overall_rating': user_rating['rating'],  # 1-5
            'specific_feedback': {
                'story_quality': user_rating.get('story', 3),
                'art_quality': user_rating.get('art', 3),
                'phase2_character': user_rating.get('characters', 3),
                'layout_quality': user_rating.get('layout', 3)
            },
            'comments': user_rating.get('comments', ''),
            'timestamp': datetime.now().isoformat()
        }
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä¿å­˜
        await self.quality_history.save_feedback(feedback_data)
        
        # ä½è©•ä¾¡ã®å ´åˆã€è©³ç´°åˆ†æ
        if user_rating['rating'] < 3:
            await self.analyze_low_rating(manga_id, feedback_data)
    
    async def analyze_quality_trends(self) -> dict:
        """
        å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        """
        # éå»30æ—¥ã®å“è³ªãƒ‡ãƒ¼ã‚¿å–å¾—
        recent_quality = await self.quality_history.get_recent_quality_data(30)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trends = {
            'average_quality_trend': self.calculate_quality_trend(recent_quality),
            'phase_performance_trends': self.analyze_phase_trends(recent_quality),
            'user_satisfaction_trend': self.calculate_satisfaction_trend(recent_quality),
            'improvement_opportunities': self.identify_improvement_areas(recent_quality)
        }
        
        return trends
    
    async def update_quality_models(self):
        """
        å“è³ªè©•ä¾¡ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•æ›´æ–°
        """
        # éå»ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’
        training_data = await self.quality_history.get_training_data()
        
        # å“è³ªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ›´æ–°
        updated_models = await self.improvement_engine.retrain_quality_models(training_data)
        
        # A/B ãƒ†ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ
        ab_test_results = await self.run_quality_model_ab_test(updated_models)
        
        # å„ªç§€ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
        if ab_test_results['new_model_better']:
            await self.deploy_updated_quality_models(updated_models)
```

### 6.3 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ›´æ–°æˆ¦ç•¥

#### å‹•çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆç®¡ç†
```python
class DynamicTestSuite:
    def __init__(self):
        self.test_generator = AITestCaseGenerator()
        self.quality_analyzer = QualityFeedbackLoop()
        
    async def update_test_suite_weekly(self):
        """
        é€±æ¬¡ã§ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’æ›´æ–°
        """
        # 1. å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        quality_trends = await self.quality_analyzer.analyze_quality_trends()
        
        # 2. å¼±ç‚¹é ˜åŸŸç‰¹å®š
        weak_areas = quality_trends['improvement_opportunities']
        
        # 3. æ–°ã—ã„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ
        new_test_cases = []
        for area in weak_areas:
            area_tests = await self.test_generator.generate_targeted_tests(area)
            new_test_cases.extend(area_tests)
        
        # 4. å¤ã„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è©•ä¾¡
        outdated_tests = await self.identify_outdated_tests()
        
        # 5. ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆæ›´æ–°
        await self.update_test_database(new_test_cases, outdated_tests)
        
        return {
            'added_tests': len(new_test_cases),
            'removed_tests': len(outdated_tests),
            'total_tests': await self.get_total_test_count(),
            'coverage_improvement': await self.calculate_coverage_improvement()
        }
    
    async def generate_regression_tests(self, bug_reports: list) -> list:
        """
        ãƒã‚°ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ã®å›å¸°ãƒ†ã‚¹ãƒˆç”Ÿæˆ
        """
        regression_tests = []
        
        for bug in bug_reports:
            # ãƒã‚°å†ç¾ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ
            test_case = await self.create_bug_reproduction_test(bug)
            
            # ä¿®æ­£æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ  
            verification_test = await self.create_fix_verification_test(bug)
            
            regression_tests.extend([test_case, verification_test])
        
        return regression_tests

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ 
test_case_schema = {
    'test_id': 'string',
    'category': 'adventure|romance|comedy|action|slice_of_life',
    'input_text': 'string',
    'expected_output': 'dict',
    'quality_threshold': 'float',
    'created_by': 'ai|human|feedback',
    'creation_date': 'datetime',
    'last_executed': 'datetime',
    'execution_count': 'int',
    'success_rate': 'float',
    'tags': 'list[string]'
}
```

---

## 7. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ

### 7.1 è‘—ä½œæ¨©ä¿è­·ãƒ†ã‚¹ãƒˆ

#### è‘—ä½œæ¨©æ¤œå‡ºãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
```python
class CopyrightProtectionTester:
    def __init__(self):
        self.copyright_detector = CopyrightProtection()
        self.test_cases = self.load_copyright_test_cases()
        
    async def test_copyright_detection_accuracy(self) -> dict:
        """
        è‘—ä½œæ¨©æ¤œå‡ºç²¾åº¦ãƒ†ã‚¹ãƒˆ
        """
        test_results = []
        
        for test_case in self.test_cases:
            result = await self.copyright_detector.check_input_text(test_case['input'])
            
            test_result = {
                'test_id': test_case['id'],
                'input': test_case['input'],
                'expected_violation': test_case['should_be_blocked'],
                'actual_violation': not result['is_safe'],
                'correct_detection': (test_case['should_be_blocked'] == (not result['is_safe'])),
                'confidence': result['confidence']
            }
            
            test_results.append(test_result)
        
        # ç²¾åº¦è¨ˆç®—
        correct_detections = sum(1 for r in test_results if r['correct_detection'])
        accuracy = correct_detections / len(test_results)
        
        return {
            'accuracy': accuracy,
            'total_tests': len(test_results),
            'correct_detections': correct_detections,
            'false_positives': self.count_false_positives(test_results),
            'false_negatives': self.count_false_negatives(test_results),
            'passes_accuracy_target': accuracy >= 0.95
        }
    
    def load_copyright_test_cases(self) -> list:
        """
        è‘—ä½œæ¨©ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
        """
        return [
            # æ˜ç¢ºãªè‘—ä½œæ¨©ä¾µå®³
            {'id': 'cp001', 'input': 'ãƒ«ãƒ•ã‚£ãŒæµ·è³Šç‹ã«ãªã‚‹ç‰©èª', 'should_be_blocked': True},
            {'id': 'cp002', 'input': 'ãƒŠãƒ«ãƒˆãŒç«å½±ã«ãªã‚‹', 'should_be_blocked': True},
            
            # å¢ƒç•Œã‚±ãƒ¼ã‚¹
            {'id': 'cp003', 'input': 'æµ·è³Šã®å†’é™ºç‰©èª', 'should_be_blocked': False},
            {'id': 'cp004', 'input': 'å¿è€…ã®æˆé•·ç‰©èª', 'should_be_blocked': False},
            
            # å¾®å¦™ãªã‚±ãƒ¼ã‚¹  
            {'id': 'cp005', 'input': 'éº¦ã‚ã‚‰å¸½å­ã‚’ã‹ã¶ã£ãŸæµ·è³Š', 'should_be_blocked': True},
            {'id': 'cp006', 'input': 'å¸½å­ã‚’ã‹ã¶ã£ãŸæµ·è³Š', 'should_be_blocked': False},
        ]
```

### 7.2 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ†ã‚¹ãƒˆ

#### ä¸é©åˆ‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡ºãƒ†ã‚¹ãƒˆ
```python
class ContentFilterTester:
    async def test_content_filtering_accuracy(self) -> dict:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç²¾åº¦ãƒ†ã‚¹ãƒˆ
        """
        test_cases = [
            # æš´åŠ›çš„è¡¨ç¾
            {'text': 'ä¸»äººå…¬ãŒæ•µã‚’æ®ºå®³ã™ã‚‹', 'should_block': True, 'category': 'violence'},
            {'text': 'ä¸»äººå…¬ãŒæ•µã¨æˆ¦ã†', 'should_block': False, 'category': 'violence'},
            
            # æ€§çš„è¡¨ç¾
            {'text': 'ã‚»ã‚¯ã‚·ãƒ¼ãªå¥³æ€§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'should_block': True, 'category': 'sexual'},
            {'text': 'ç¾ã—ã„å¥³æ€§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'should_block': False, 'category': 'sexual'},
            
            # å·®åˆ¥è¡¨ç¾
            {'text': 'æ•µã‚’ãƒã‚«ã¨å‘¼ã¶', 'should_block': True, 'category': 'discrimination'},
            {'text': 'æ•µã¨çŸ¥æµæ¯”ã¹ã‚’ã™ã‚‹', 'should_block': False, 'category': 'discrimination'}
        ]
        
        filter_results = []
        
        for case in test_cases:
            result = await self.content_filter.filter_input_text(case['text'])
            
            filter_results.append({
                'test_case': case,
                'filter_result': result,
                'correct_decision': (case['should_block'] == (not result['is_safe']))
            })
        
        accuracy = sum(1 for r in filter_results if r['correct_decision']) / len(filter_results)
        
        return {
            'filter_accuracy': accuracy,
            'passes_target': accuracy >= 0.95,
            'category_breakdown': self.analyze_by_category(filter_results)
        }
```

### 7.3 èªè¨¼ãƒ»èªå¯ãƒ†ã‚¹ãƒˆ

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
```python
class SecurityTestSuite:
    @pytest.mark.asyncio
    async def test_authentication_flow(self):
        """
        èªè¨¼ãƒ•ãƒ­ãƒ¼ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
        """
        # 1. æ­£å¸¸ãƒ­ã‚°ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
        valid_login = await self.client.post('/api/auth/login', json={
            'email': 'test@example.com', 
            'password': 'validpassword'
        })
        assert valid_login.status_code == 200
        
        # 2. ä¸æ­£ãƒ­ã‚°ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
        invalid_login = await self.client.post('/api/auth/login', json={
            'email': 'test@example.com',
            'password': 'wrongpassword'
        })
        assert invalid_login.status_code == 401
        
        # 3. ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ†ã‚¹ãƒˆ
        for _ in range(6):  # 5å›åˆ¶é™ã‚’è¶…ãˆã‚‹
            await self.client.post('/api/auth/login', json={
                'email': 'test@example.com',
                'password': 'wrongpassword'
            })
        
        rate_limited = await self.client.post('/api/auth/login', json={
            'email': 'test@example.com',
            'password': 'wrongpassword'
        })
        assert rate_limited.status_code == 429
    
    @pytest.mark.asyncio
    async def test_authorization_levels(self):
        """
        èªå¯ãƒ¬ãƒ™ãƒ«ãƒ†ã‚¹ãƒˆ
        """
        # freeãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ
        free_token = await self.get_user_token('free')
        
        # 1å›ç›®ç”Ÿæˆï¼ˆæˆåŠŸï¼‰
        response1 = await self.client.post('/api/manga/generate', 
            headers={'Authorization': f'Bearer {free_token}'},
            json={'text': 'ãƒ†ã‚¹ãƒˆç‰©èª1', 'style': 'å°‘å¹´æ¼«ç”»'}
        )
        assert response1.status_code == 202
        
        # 2å›ç›®ç”Ÿæˆï¼ˆæ—¥æ¬¡åˆ¶é™ã§æ‹’å¦ï¼‰
        response2 = await self.client.post('/api/manga/generate',
            headers={'Authorization': f'Bearer {free_token}'},
            json={'text': 'ãƒ†ã‚¹ãƒˆç‰©èª2', 'style': 'å°‘å¹´æ¼«ç”»'}
        )
        assert response2.status_code == 429
```

---

## 8. CI/CDçµ±åˆ

### 8.1 ãƒ†ã‚¹ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

#### Cloud Build ãƒ†ã‚¹ãƒˆè¨­å®š
```yaml
# cloudbuild-test.yaml
steps:
  # 1. ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
  - name: 'python:3.11'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio
    
  # 2. å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  - name: 'python:3.11'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pytest tests/unit/ \
          --cov=./ \
          --cov-report=xml \
          --cov-fail-under=80 \
          --junitxml=test-results.xml
    
  # 3. çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  - name: 'python:3.11'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pytest tests/integration/ \
          --timeout=300 \
          --junitxml=integration-results.xml
    env:
      - 'REDIS_HOST=test-redis'
      - 'DB_HOST=test-db'
    
  # 4. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
  - name: 'python:3.11'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pytest tests/security/ \
          --junitxml=security-results.xml
    
  # 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ç‰ˆï¼‰
  - name: 'python:3.11'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        locust --headless \
          --users 10 \
          --spawn-rate 2 \
          --run-time 2m \
          --host http://test-service \
          --html performance-report.html
    
  # 6. AIå“è³ªãƒ†ã‚¹ãƒˆ
  - name: 'python:3.11'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        python tests/ai_quality/run_quality_tests.py \
          --test-suite basic \
          --output quality-report.json
    env:
      - 'GEMINI_API_KEY=${_GEMINI_TEST_KEY}'
      - 'IMAGEN_API_KEY=${_IMAGEN_TEST_KEY}'
    
  # 7. ãƒ†ã‚¹ãƒˆçµæœçµ±åˆ
  - name: 'gcr.io/cloud-builders/gsutil'
    args:
      - 'cp'
      - '*.xml'
      - '*.html' 
      - '*.json'
      - 'gs://manga-test-results/${BUILD_ID}/'

substitutions:
  _GEMINI_TEST_KEY: projects/PROJECT_ID/secrets/gemini-test-key/versions/latest
  _IMAGEN_TEST_KEY: projects/PROJECT_ID/secrets/imagen-test-key/versions/latest

timeout: 1800s  # 30åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
```

### 8.2 å“è³ªã‚²ãƒ¼ãƒˆè¨­å®š

#### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå“è³ªã‚²ãƒ¼ãƒˆ
```python
class DeploymentQualityGate:
    def __init__(self):
        self.quality_requirements = {
            'unit_test_coverage': 0.80,           # 80%ä»¥ä¸Š
            'integration_test_success': 1.0,     # 100%æˆåŠŸ
            'security_test_success': 1.0,        # 100%æˆåŠŸ
            'ai_quality_average': 0.85,          # 85%ä»¥ä¸Š
            'performance_degradation': 0.20      # 20%ä»¥ä¸‹ã®æ€§èƒ½åŠ£åŒ–
        }
    
    async def evaluate_deployment_readiness(self, test_results: dict) -> dict:
        """
        ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæº–å‚™å®Œäº†åˆ¤å®š
        """
        gate_results = {}
        all_passed = True
        
        # å„å“è³ªã‚²ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        for requirement, threshold in self.quality_requirements.items():
            actual_value = test_results.get(requirement, 0)
            
            if requirement == 'performance_degradation':
                # æ€§èƒ½åŠ£åŒ–ã¯å°‘ãªã„æ–¹ãŒè‰¯ã„
                passed = actual_value <= threshold
            else:
                # ãã®ä»–ã¯é«˜ã„æ–¹ãŒè‰¯ã„
                passed = actual_value >= threshold
            
            gate_results[requirement] = {
                'required': threshold,
                'actual': actual_value,
                'passed': passed
            }
            
            if not passed:
                all_passed = False
        
        return {
            'deployment_approved': all_passed,
            'gate_results': gate_results,
            'failure_reasons': [
                req for req, result in gate_results.items() 
                if not result['passed']
            ]
        }
```

### 8.3 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰¿èª

#### è‡ªå‹•ãƒ»æ‰‹å‹•æ‰¿èªãƒ•ãƒ­ãƒ¼
```python
class DeploymentApprovalManager:
    async def process_deployment_request(self, build_id: str, test_results: dict) -> dict:
        """
        ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰¿èªå‡¦ç†
        """
        # å“è³ªã‚²ãƒ¼ãƒˆè©•ä¾¡
        quality_gate = DeploymentQualityGate()
        gate_result = await quality_gate.evaluate_deployment_readiness(test_results)
        
        if gate_result['deployment_approved']:
            # è‡ªå‹•æ‰¿èª
            approval_result = await self.auto_approve_deployment(build_id)
        else:
            # æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼å¿…è¦
            approval_result = await self.request_manual_review(build_id, gate_result)
        
        return approval_result
    
    async def auto_approve_deployment(self, build_id: str) -> dict:
        """
        è‡ªå‹•æ‰¿èªå‡¦ç†
        """
        # Productionç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ
        deployment_result = await self.trigger_production_deployment(build_id)
        
        # æ‰¿èªãƒ­ã‚°è¨˜éŒ²
        await self.log_approval_event({
            'build_id': build_id,
            'approval_type': 'automatic',
            'approver': 'system',
            'timestamp': datetime.now().isoformat()
        })
        
        return {
            'status': 'approved',
            'deployment_id': deployment_result['deployment_id'],
            'approval_type': 'automatic'
        }
    
    async def request_manual_review(self, build_id: str, gate_result: dict) -> dict:
        """
        æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦æ±‚
        """
        # é–‹ç™ºãƒãƒ¼ãƒ ã«é€šçŸ¥
        await self.notify_review_required(build_id, gate_result['failure_reasons'])
        
        return {
            'status': 'pending_review',
            'build_id': build_id,
            'required_reviews': gate_result['failure_reasons'],
            'review_url': f'https://console.cloud.google.com/cloud-build/builds/{build_id}'
        }
```

---

## 9. ãƒ†ã‚¹ãƒˆç’°å¢ƒç®¡ç†

### 9.1 ç’°å¢ƒæ§‹æˆ

#### ãƒ†ã‚¹ãƒˆç’°å¢ƒè¨­å®š
```yaml
Test Environments:
  Unit Test Environment:
    Database: SQLite (in-memory)
    Cache: Redis (mock)
    External APIs: Mock servers
    
  Integration Test Environment:  
    Database: Cloud SQL (test instance)
    Cache: Memory Store Redis (test instance)
    External APIs: Sandbox APIs
    
  E2E Test Environment:
    Database: Cloud SQL (staging replica)
    Cache: Memory Store Redis (staging)
    External APIs: Production APIs (limited quota)
    
  Performance Test Environment:
    Infrastructure: Production-like scaling
    Database: Cloud SQL (performance instance)
    Load Balancer: Enabled
    Monitoring: Full monitoring stack
```

#### ãƒ†ã‚¹ãƒˆç’°å¢ƒãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
```bash
#!/bin/bash
# setup-test-environment.sh

echo "=== ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ==="

# 1. ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
gcloud sql instances create manga-test-db \
  --database-version=POSTGRES_15 \
  --tier=db-f1-micro \
  --region=asia-northeast1 \
  --no-backup

# 2. ãƒ†ã‚¹ãƒˆç”¨Redisä½œæˆ
gcloud redis instances create manga-test-redis \
  --size=1 \
  --region=asia-northeast1 \
  --redis-version=redis_7_0

# 3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥
psql -h $TEST_DB_HOST -U postgres -d manga_test < test_data.sql

# 4. Cloud Run ãƒ†ã‚¹ãƒˆã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ãƒ—ãƒ­ã‚¤
for phase in {1..8}; do
  gcloud run deploy test-phase${phase}-service \
    --image gcr.io/$PROJECT_ID/phase${phase}:test \
    --region asia-northeast1 \
    --no-allow-unauthenticated \
    --set-env-vars="ENVIRONMENT=test"
done

echo "=== ãƒ†ã‚¹ãƒˆç’°å¢ƒæº–å‚™å®Œäº† ==="
```

---


---

## æ”¹è¨‚å±¥æ­´

| ç‰ˆæ•° | æ—¥ä»˜ | å¤‰æ›´å†…å®¹ | æ‹…å½“è€… |
|------|------|----------|--------|
| 1.0 | 2025-01-20 | åˆç‰ˆä½œæˆï¼ˆåŸºæœ¬å“è³ªä¿è¨¼ï¼‹AIè‡ªå‹•ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆï¼‰ | Claude Code |

---

**æ–‡æ›¸æ‰¿èª**
- QAã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢: [ç½²å] æ—¥ä»˜: [æ—¥ä»˜]
- ãƒ†ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼: [ç½²å] æ—¥ä»˜: [æ—¥ä»˜]
- å“è³ªä¿è¨¼è²¬ä»»è€…: [ç½²å] æ—¥ä»˜: [æ—¥ä»˜]