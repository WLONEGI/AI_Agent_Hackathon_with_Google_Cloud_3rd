# AI漫画生成サービス 運用監視設計書

**文書管理情報**
- 文書ID: OPS-DOC-001
- 作成日: 2025-01-20
- 版数: 1.0
- 承認者: 根岸祐樹
- 関連文書: INF-DOC-001（インフラ設計書）、TEST-DOC-001（テスト設計書）

## 目次

- [1. 運用概要](#1-運用概要)
  - [1.1 運用方針](#11-運用方針)
  - [1.2 SLA定義](#12-sla定義)
- [2. 監視設計](#2-監視設計)
  - [2.1 監視項目](#21-監視項目)
  - [2.2 メトリクス収集](#22-メトリクス収集)
  - [2.3 ダッシュボード設計](#23-ダッシュボード設計)
- [3. アラート設計](#3-アラート設計)
  - [3.1 アラートレベル](#31-アラートレベル)
  - [3.2 通知設定](#32-通知設定)
  - [3.3 エスカレーション](#33-エスカレーション)
- [4. ログ管理](#4-ログ管理)
  - [4.1 ログ収集戦略](#41-ログ収集戦略)
  - [4.2 ログ分析](#42-ログ分析)
  - [4.3 ログ保持ポリシー](#43-ログ保持ポリシー)
- [5. インシデント対応](#5-インシデント対応)
  - [5.1 インシデント分類](#51-インシデント分類)
  - [5.2 対応手順](#52-対応手順)
  - [5.3 ポストモーテム](#53-ポストモーテム)
- [6. 定期メンテナンス](#6-定期メンテナンス)
  - [6.1 メンテナンス計画](#61-メンテナンス計画)
  - [6.2 アップデート戦略](#62-アップデート戦略)
  - [6.3 容量管理](#63-容量管理)
- [7. 災害復旧](#7-災害復旧)
  - [7.1 バックアップ戦略](#71-バックアップ戦略)
  - [7.2 復旧手順](#72-復旧手順)
  - [7.3 事業継続計画](#73-事業継続計画)
- [8. 運用自動化](#8-運用自動化)
  - [8.1 自動化ツール](#81-自動化ツール)
  - [8.2 運用スクリプト](#82-運用スクリプト)
  - [8.3 運用効率化](#83-運用効率化)
- [9. キャパシティプランニング](#9-キャパシティプランニング)
- [10. 運用チーム体制](#10-運用チーム体制)

---

## 1. 運用概要

### 1.1 運用方針

#### 基本原則
| 原則 | 内容 | 実装レベル |
|------|------|----------|
| 高可用性重視 | SLA 99.5%達成を最優先 | 基本 |
| 予防的運用 | 問題発生前の早期検出・対処 | 基本 |
| 自動化推進 | 手動作業の最小化 | 基本 |
| 透明性確保 | 運用状況の可視化・共有 | 基本 |

#### 運用体制
```yaml
Operation Team Structure:
  SRE Engineer (1名):
    - システム信頼性確保
    - 監視・アラート管理
    - インシデント初動対応
    
  DevOps Engineer (1名):
    - CI/CD パイプライン管理
    - インフラ自動化
    - デプロイメント支援
    
  On-call Rotation:
    - 平日: 9:00-18:00 (営業時間)
    - 休日・夜間: オンコール対応
    - エスカレーション: 重大インシデント時
```

### 1.2 SLA定義

#### サービスレベル目標
```yaml
Service Level Objectives:
  Availability:
    Target: 99.5%
    Measurement: Uptime monitoring
    Downtime Budget: 3.6 hours/month
    
  Performance:
    API Response Time: 95% < 2 seconds
    Manga Generation: 95% < 10 minutes
    Page Load Time: 95% < 3 seconds
    
  Reliability:
    Error Rate: < 0.5%
    Success Rate: > 99.5%
    Data Durability: 99.999%
    
  AI Quality:
    Generation Success Rate: > 85%
    User Satisfaction: > 4.0/5.0
    Content Safety: > 99%
```

#### SLA計算方法
```python
class SLACalculator:
    def __init__(self):
        self.target_availability = 0.995  # 99.5%
        
    async def calculate_monthly_sla(self, month: str) -> dict:
        """
        月次SLA計算
        """
        # ダウンタイム収集
        downtime_events = await self.get_downtime_events(month)
        total_downtime = sum(event['duration'] for event in downtime_events)
        
        # 月の総時間
        total_hours = self.get_month_hours(month)
        
        # 可用性計算
        uptime_hours = total_hours - (total_downtime / 3600)  # 秒→時間
        availability = uptime_hours / total_hours
        
        # SLA達成判定
        sla_met = availability >= self.target_availability
        remaining_budget = (total_hours * (1 - self.target_availability)) - (total_downtime / 3600)
        
        return {
            'month': month,
            'availability': availability,
            'uptime_hours': uptime_hours,
            'downtime_hours': total_downtime / 3600,
            'sla_met': sla_met,
            'remaining_error_budget': max(0, remaining_budget),
            'downtime_events': downtime_events
        }
```

---

## 2. 監視設計

### 2.1 監視項目

#### システム監視項目
```yaml
System Monitoring:
  Infrastructure:
    - Cloud Run instances (CPU, Memory, Request count)
    - Cloud SQL (Connections, Query performance, Storage)
    - Redis (Memory usage, Hit rate, Connections)
    - Cloud Storage (Request count, Transfer volume)
    
  Application:
    - API response times (per endpoint)
    - Error rates (per service)
    - Request throughput
    - Queue lengths (Pub/Sub)
    
  AI Processing:
    - Phase completion rates
    - Phase processing times
    - AI API usage (quota consumption)
    - Quality scores (per phase)
    
  Business Metrics:
    - Active users
    - Manga generation requests
    - User satisfaction scores
    - Revenue metrics (subscription, usage)
```

#### 監視実装
```python
class ServiceMonitor:
    def __init__(self):
        self.monitoring_client = monitoring_v3.MetricServiceClient()
        self.project_name = f"projects/{PROJECT_ID}"
        
    async def collect_custom_metrics(self):
        """
        カスタムメトリクス収集
        """
        while True:
            try:
                # AI処理品質メトリクス
                await self.collect_ai_quality_metrics()
                
                # ビジネスメトリクス
                await self.collect_business_metrics()
                
                # パフォーマンスメトリクス
                await self.collect_performance_metrics()
                
                # 30秒間隔で収集
                await asyncio.sleep(30)
                
            except Exception as e:
                logger.error(f"Metrics collection error: {e}")
                await asyncio.sleep(60)
    
    async def collect_ai_quality_metrics(self):
        """
        AI品質メトリクス収集
        """
        # 過去5分間の品質スコア取得
        quality_scores = await self.get_recent_quality_scores(minutes=5)
        
        if quality_scores:
            # 平均品質スコア
            avg_quality = sum(quality_scores) / len(quality_scores)
            
            # Cloud Monitoring に送信
            await self.send_custom_metric(
                'ai.quality.average_score',
                avg_quality,
                {'service': 'manga_generation'}
            )
            
            # フェーズ別品質スコア
            for phase in range(1, 9):
                phase_scores = [s for s in quality_scores if s['phase'] == phase]
                if phase_scores:
                    phase_avg = sum(s['score'] for s in phase_scores) / len(phase_scores)
                    await self.send_custom_metric(
                        'ai.quality.phase_score',
                        phase_avg,
                        {'phase': str(phase)}
                    )
```

### 2.2 メトリクス収集

#### Prometheus メトリクス定義
```python
from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry

# カスタムメトリクス定義
class CustomMetrics:
    def __init__(self):
        self.registry = CollectorRegistry()
        
        # リクエストカウンター
        self.request_counter = Counter(
            'manga_requests_total',
            'Total manga generation requests',
            ['phase', 'status', 'user_type'],
            registry=self.registry
        )
        
        # 処理時間ヒストグラム
        self.processing_time = Histogram(
            'manga_processing_duration_seconds',
            'Time spent processing manga generation',
            ['phase'],
            buckets=[10, 30, 60, 120, 300, 600],  # 10秒〜10分
            registry=self.registry
        )
        
        # 品質スコアゲージ
        self.quality_score = Gauge(
            'manga_quality_score',
            'AI generation quality score',
            ['phase'],
            registry=self.registry
        )
        
        # アクティブユーザー
        self.active_users = Gauge(
            'manga_active_users',
            'Number of active users',
            ['time_period'],
            registry=self.registry
        )
        
        # API使用量
        self.api_usage = Counter(
            'external_api_calls_total',
            'External API calls',
            ['api_name', 'status'],
            registry=self.registry
        )
    
    def record_phase_completion(self, phase: int, duration: float, status: str, user_type: str):
        """
        フェーズ完了メトリクス記録
        """
        self.request_counter.labels(
            phase=str(phase), 
            status=status, 
            user_type=user_type
        ).inc()
        
        self.processing_time.labels(phase=str(phase)).observe(duration)
    
    def update_quality_score(self, phase: int, score: float):
        """
        品質スコア更新
        """
        self.quality_score.labels(phase=str(phase)).set(score)
```

### 2.3 ダッシュボード設計

#### Cloud Monitoring ダッシュボード
```json
{
  "displayName": "AI漫画生成サービス - 運用ダッシュボード",
  "mosaicLayout": {
    "tiles": [
      {
        "title": "サービス可用性",
        "xyChart": {
          "dataSets": [
            {
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"cloud_run_revision\"",
                  "aggregation": {
                    "alignmentPeriod": "60s",
                    "perSeriesAligner": "ALIGN_RATE"
                  }
                }
              }
            }
          ],
          "yAxis": {"label": "可用性 (%)"},
          "thresholds": [
            {"value": 0.995, "color": "GREEN", "direction": "ABOVE"}
          ]
        }
      },
      {
        "title": "AI処理品質スコア",
        "scorecard": {
          "timeSeriesQuery": {
            "timeSeriesFilter": {
              "filter": "metric.type=\"custom.googleapis.com/ai/quality/average_score\""
            }
          },
          "sparkChartView": {"sparkChartType": "SPARK_LINE"},
          "thresholds": [
            {"value": 0.85, "color": "YELLOW", "direction": "BELOW"},
            {"value": 0.75, "color": "RED", "direction": "BELOW"}
          ]
        }
      },
      {
        "title": "API応答時間 (95パーセンタイル)",
        "xyChart": {
          "dataSets": [
            {
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"cloud_run_revision\" AND metric.type=\"run.googleapis.com/request_latencies\"",
                  "aggregation": {
                    "alignmentPeriod": "60s",
                    "perSeriesAligner": "ALIGN_DELTA",
                    "crossSeriesReducer": "REDUCE_PERCENTILE_95"
                  }
                }
              }
            }
          ],
          "yAxis": {"label": "レイテンシ (ms)"},
          "thresholds": [
            {"value": 2000, "color": "YELLOW", "direction": "ABOVE"},
            {"value": 5000, "color": "RED", "direction": "ABOVE"}
          ]
        }
      }
    ]
  }
}
```

---

## 3. アラート設計

### 3.1 アラートレベル

#### 重要度別アラート分類
```yaml
Alert Levels:
  Critical (P1):
    - Service completely down
    - Data loss detected
    - Security breach
    Response Time: 15 minutes
    Notification: Phone, SMS, Email, Slack
    
  High (P2):
    - High error rate (>5%)
    - API quota exhausted
    - Database connection failure
    Response Time: 1 hour
    Notification: SMS, Email, Slack
    
  Medium (P3):
    - Performance degradation
    - AI quality score drop
    - Resource utilization high
    Response Time: 4 hours
    Notification: Email, Slack
    
  Low (P4):
    - Minor configuration drift
    - Log volume increase
    - Non-critical warnings
    Response Time: 24 hours
    Notification: Email
```

### 3.2 通知設定

#### アラート通知実装
```python
class AlertManager:
    def __init__(self):
        self.notification_channels = {
            'email': EmailNotifier(),
            'slack': SlackNotifier(), 
            'sms': SMSNotifier(),
            'phone': PhoneNotifier()
        }
        
    async def process_alert(self, alert: dict):
        """
        アラート処理
        """
        severity = alert['severity']
        
        # 重要度に応じた通知チャネル選択
        channels = self.get_notification_channels(severity)
        
        # 通知メッセージ生成
        message = self.format_alert_message(alert)
        
        # 並列通知送信
        tasks = [
            self.send_notification(channel, message, alert)
            for channel in channels
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 通知失敗の処理
        failed_notifications = [
            (channels[i], result) for i, result in enumerate(results)
            if isinstance(result, Exception)
        ]
        
        if failed_notifications:
            await self.handle_notification_failures(failed_notifications, alert)
    
    def get_notification_channels(self, severity: str) -> list:
        """
        重要度別通知チャネル
        """
        channel_map = {
            'critical': ['phone', 'sms', 'email', 'slack'],
            'high': ['sms', 'email', 'slack'],
            'medium': ['email', 'slack'],
            'low': ['email']
        }
        return channel_map.get(severity, ['email'])

# Slack通知実装
class SlackNotifier:
    def __init__(self):
        self.webhook_url = os.getenv('SLACK_WEBHOOK_URL')
        
    async def send_alert(self, message: dict, alert: dict):
        """
        Slackアラート送信
        """
        color_map = {
            'critical': '#FF0000',  # 赤
            'high': '#FF8C00',      # オレンジ
            'medium': '#FFD700',    # 黄
            'low': '#87CEEB'        # 青
        }
        
        payload = {
            "username": "AI漫画監視Bot",
            "icon_emoji": ":warning:",
            "attachments": [
                {
                    "color": color_map.get(alert['severity'], '#87CEEB'),
                    "title": f"【{alert['severity'].upper()}】{alert['title']}",
                    "text": message['body'],
                    "fields": [
                        {"title": "サービス", "value": alert.get('service', 'Unknown'), "short": True},
                        {"title": "発生時刻", "value": alert['timestamp'], "short": True},
                        {"title": "影響", "value": alert.get('impact', 'Unknown'), "short": True}
                    ],
                    "actions": [
                        {
                            "type": "button",
                            "text": "ダッシュボード確認",
                            "url": "https://console.cloud.google.com/monitoring"
                        },
                        {
                            "type": "button", 
                            "text": "ログ確認",
                            "url": f"https://console.cloud.google.com/logs/query;query={alert.get('log_query', '')}"
                        }
                    ]
                }
            ]
        }
        
        async with aiohttp.ClientSession() as session:
            await session.post(self.webhook_url, json=payload)
```

### 3.3 エスカレーション

#### エスカレーション matrix
```python
class EscalationManager:
    def __init__(self):
        self.escalation_rules = {
            'critical': {
                'immediate': ['sre_engineer', 'devops_engineer'],
                '15_min': ['tech_lead'],
                '30_min': ['cto'],
                '60_min': ['ceo']
            },
            'high': {
                'immediate': ['sre_engineer'],
                '2_hour': ['tech_lead'],
                '4_hour': ['cto']
            },
            'medium': {
                '4_hour': ['sre_engineer'],
                '24_hour': ['tech_lead']
            }
        }
    
    async def handle_escalation(self, alert: dict, elapsed_minutes: int):
        """
        エスカレーション処理
        """
        severity = alert['severity']
        escalation_rule = self.escalation_rules.get(severity, {})
        
        for time_threshold, recipients in escalation_rule.items():
            threshold_minutes = self.parse_time_threshold(time_threshold)
            
            if elapsed_minutes >= threshold_minutes:
                # まだ通知していない場合のみ
                if not await self.already_notified(alert['id'], time_threshold):
                    await self.notify_escalation(alert, recipients, time_threshold)
                    await self.mark_escalation_sent(alert['id'], time_threshold)
    
    def parse_time_threshold(self, threshold: str) -> int:
        """
        時間閾値をパース
        """
        if threshold == 'immediate':
            return 0
        elif threshold.endswith('_min'):
            return int(threshold.split('_')[0])
        elif threshold.endswith('_hour'):
            return int(threshold.split('_')[0]) * 60
        return 0
```

---

## 4. ログ管理

### 4.1 ログ収集戦略

#### 構造化ログ設計
```python
import structlog
from google.cloud import logging as cloud_logging

class StructuredLogger:
    def __init__(self, service_name: str):
        # Google Cloud Logging設定
        cloud_logging.Client().setup_logging()
        
        # 構造化ログ設定
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )
        
        self.logger = structlog.get_logger(service=service_name)
    
    def log_request_start(self, request_id: str, user_id: str, request_data: dict):
        """
        リクエスト開始ログ
        """
        self.logger.info(
            "manga_generation_started",
            request_id=request_id,
            user_id=user_id,
            text_length=len(request_data.get('text', '')),
            style=request_data.get('style'),
            estimated_pages=request_data.get('pages'),
            user_type='free'  # TODO: 実際のユーザータイプ
        )
    
    def log_phase_completion(self, request_id: str, phase: int, duration: float, quality_score: float):
        """
        フェーズ完了ログ
        """
        self.logger.info(
            "phase_completed",
            request_id=request_id,
            phase=phase,
            duration_seconds=duration,
            quality_score=quality_score,
            passes_quality_gate=quality_score >= 0.85
        )
    
    def log_error(self, request_id: str, error: Exception, context: dict):
        """
        エラーログ
        """
        self.logger.error(
            "processing_error",
            request_id=request_id,
            error_type=type(error).__name__,
            error_message=str(error),
            context=context,
            exc_info=True
        )
```

### 4.2 ログ分析

#### 自動ログ分析
```python
class LogAnalyzer:
    def __init__(self):
        self.bigquery_client = bigquery.Client()
        
    async def analyze_error_patterns(self, hours: int = 24) -> dict:
        """
        エラーパターン分析
        """
        query = f"""
        SELECT 
            JSON_EXTRACT_SCALAR(jsonPayload, '$.error_type') as error_type,
            JSON_EXTRACT_SCALAR(jsonPayload, '$.phase') as phase,
            COUNT(*) as error_count,
            ARRAY_AGG(JSON_EXTRACT_SCALAR(jsonPayload, '$.error_message') LIMIT 5) as sample_messages
        FROM `{PROJECT_ID}.logging.cloud_run_logs`
        WHERE 
            timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {hours} HOUR)
            AND severity = 'ERROR'
            AND jsonPayload.error_type IS NOT NULL
        GROUP BY error_type, phase
        ORDER BY error_count DESC
        """
        
        results = self.bigquery_client.query(query).result()
        
        error_analysis = []
        for row in results:
            error_analysis.append({
                'error_type': row.error_type,
                'phase': row.phase,
                'count': row.error_count,
                'sample_messages': row.sample_messages
            })
        
        return {
            'period_hours': hours,
            'total_errors': sum(e['count'] for e in error_analysis),
            'error_patterns': error_analysis,
            'top_error_types': error_analysis[:10]
        }
    
    async def analyze_performance_trends(self, days: int = 7) -> dict:
        """
        パフォーマンストレンド分析
        """
        query = f"""
        SELECT 
            DATE(timestamp) as date,
            JSON_EXTRACT_SCALAR(jsonPayload, '$.phase') as phase,
            AVG(CAST(JSON_EXTRACT_SCALAR(jsonPayload, '$.duration_seconds') AS FLOAT64)) as avg_duration,
            COUNT(*) as request_count
        FROM `{PROJECT_ID}.logging.cloud_run_logs`
        WHERE 
            timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {days} DAY)
            AND jsonPayload.duration_seconds IS NOT NULL
        GROUP BY date, phase
        ORDER BY date DESC, phase
        """
        
        results = self.bigquery_client.query(query).result()
        
        return self.format_performance_trends(results)
```

### 4.3 ログ保持ポリシー

#### ログ保持・アーカイブ戦略
```yaml
Log Retention Policy:
  Application Logs:
    Cloud Logging: 30 days
    BigQuery Export: 1 year
    Cold Storage: 7 years
    
  Audit Logs:
    Cloud Logging: 90 days
    BigQuery Export: 3 years
    Cold Storage: 10 years
    
  Performance Logs:
    Cloud Logging: 7 days
    BigQuery Export: 6 months
    Cold Storage: 2 years
    
  Error Logs:
    Cloud Logging: 30 days
    BigQuery Export: 2 years
    Cold Storage: 5 years

Log Export Configuration:
  BigQuery Dataset: manga_service_logs
  Tables:
    - application_logs
    - audit_logs  
    - performance_logs
    - error_logs
  
  Partitioning: Daily partitioning by timestamp
  Clustering: service, severity, request_id
```

---

## 5. インシデント対応

### 5.1 インシデント分類

#### インシデント分類マトリクス
| インシデント種別 | 例 | 優先度 | 対応時間 |
|----------------|---|--------|---------|
| 完全サービス停止 | Cloud Run全サービスダウン | P1 | 15分 |
| 主要機能停止 | 漫画生成機能停止 | P1 | 15分 |
| データ消失 | データベース障害 | P1 | 15分 |
| セキュリティ侵害 | 不正アクセス検出 | P1 | 15分 |
| 性能大幅劣化 | 応答時間10倍悪化 | P2 | 1時間 |
| AI品質劣化 | 品質スコア50%以下 | P2 | 1時間 |
| 部分機能停止 | ギャラリー機能停止 | P3 | 4時間 |
| 軽微な性能劣化 | 応答時間2倍悪化 | P3 | 4時間 |

### 5.2 対応手順

#### インシデント対応フロー
```python
class IncidentResponseManager:
    def __init__(self):
        self.incident_db = IncidentDatabase()
        self.notification_manager = AlertManager()
        
    async def handle_incident(self, alert: dict) -> str:
        """
        インシデント対応処理
        """
        # 1. インシデント登録
        incident_id = await self.create_incident(alert)
        
        # 2. 重要度判定
        severity = self.classify_incident_severity(alert)
        
        # 3. 初動対応
        await self.execute_immediate_response(incident_id, severity, alert)
        
        # 4. 担当者アサイン
        await self.assign_incident_responder(incident_id, severity)
        
        # 5. ステークホルダー通知
        await self.notify_stakeholders(incident_id, severity, alert)
        
        return incident_id
    
    async def execute_immediate_response(self, incident_id: str, severity: str, alert: dict):
        """
        初動対応実行
        """
        if severity == 'critical':
            # クリティカル時の自動対応
            await self.auto_mitigation_critical(alert)
        
        elif severity == 'high':
            # 高優先度時の自動診断
            await self.auto_diagnosis_high(alert)
        
        # 影響範囲調査
        impact_assessment = await self.assess_incident_impact(alert)
        
        # インシデント情報更新
        await self.update_incident(incident_id, {
            'impact_assessment': impact_assessment,
            'immediate_actions': await self.get_immediate_actions(alert)
        })

# 自動回復アクション
class AutoRecoveryActions:
    async def auto_mitigation_critical(self, alert: dict):
        """
        クリティカルインシデントの自動軽減
        """
        alert_type = alert.get('metric_type', '')
        
        if 'cloud_run' in alert_type:
            # Cloud Run サービス再起動
            await self.restart_cloud_run_services()
            
        elif 'database' in alert_type:
            # データベース接続プール リセット
            await self.reset_database_connections()
            
        elif 'redis' in alert_type:
            # Redis接続リセット
            await self.reset_redis_connections()
    
    async def restart_cloud_run_services(self):
        """
        Cloud Run サービス再起動
        """
        services = ['phase1-service', 'phase2-service', 'phase3-service', 
                   'phase4-service', 'phase5-service', 'phase6-service',
                   'phase7-service', 'phase8-service']
        
        for service in services:
            # 新しいリビジョンデプロイによる再起動
            subprocess.run([
                'gcloud', 'run', 'deploy', service,
                '--image', f'gcr.io/{PROJECT_ID}/{service}:latest',
                '--region', 'asia-northeast1'
            ])
```

### 5.3 ポストモーテム

#### 事後分析プロセス
```python
class PostmortemManager:
    async def create_postmortem(self, incident_id: str) -> dict:
        """
        ポストモーテム作成
        """
        incident = await self.get_incident_details(incident_id)
        
        postmortem = {
            'incident_id': incident_id,
            'title': incident['title'],
            'date': incident['occurred_at'],
            'duration': incident['resolution_time'],
            'impact': await self.calculate_incident_impact(incident),
            'timeline': await self.build_incident_timeline(incident),
            'root_cause': await self.analyze_root_cause(incident),
            'contributing_factors': await self.identify_contributing_factors(incident),
            'lessons_learned': await self.extract_lessons_learned(incident),
            'action_items': await self.generate_action_items(incident)
        }
        
        return postmortem
    
    async def generate_action_items(self, incident: dict) -> list:
        """
        改善アクションアイテム生成
        """
        action_items = []
        
        # 自動的なアクション提案
        if 'timeout' in incident['root_cause']:
            action_items.append({
                'action': 'タイムアウト値の見直し',
                'priority': 'high',
                'assignee': 'sre_team',
                'due_date': self.calculate_due_date(7)  # 1週間後
            })
        
        if 'monitoring' in incident['contributing_factors']:
            action_items.append({
                'action': '監視アラートの改善',
                'priority': 'medium',
                'assignee': 'devops_team',
                'due_date': self.calculate_due_date(14)  # 2週間後
            })
        
        return action_items
```

---

## 6. 定期メンテナンス

### 6.1 メンテナンス計画

#### 定期メンテナンススケジュール
```yaml
Maintenance Schedule:
  Daily Maintenance:
    Time: 03:00-04:00 JST
    Tasks:
      - Database backup verification
      - Log rotation
      - Temporary file cleanup
      - Metrics aggregation
    
  Weekly Maintenance:
    Time: Sunday 02:00-04:00 JST
    Tasks:
      - Security patch application
      - Performance optimization
      - Test suite update
      - Capacity planning review
    
  Monthly Maintenance:
    Time: First Sunday 01:00-05:00 JST
    Tasks:
      - Major version updates
      - Infrastructure review
      - Cost optimization
      - Security audit
```

#### メンテナンス自動化
```python
class MaintenanceAutomator:
    def __init__(self):
        self.scheduler = CloudScheduler()
        
    async def setup_maintenance_jobs(self):
        """
        定期メンテナンスジョブ設定
        """
        # 日次メンテナンス
        await self.scheduler.create_job(
            name='daily-maintenance',
            schedule='0 3 * * *',  # 毎日3時
            target=self.daily_maintenance,
            timezone='Asia/Tokyo'
        )
        
        # 週次メンテナンス
        await self.scheduler.create_job(
            name='weekly-maintenance', 
            schedule='0 2 * * 0',  # 日曜2時
            target=self.weekly_maintenance,
            timezone='Asia/Tokyo'
        )
    
    async def daily_maintenance(self):
        """
        日次メンテナンス実行
        """
        tasks = [
            self.verify_backups(),
            self.cleanup_temp_files(),
            self.rotate_logs(),
            self.update_metrics_aggregation()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # メンテナンス結果レポート
        await self.generate_maintenance_report('daily', results)
    
    async def cleanup_temp_files(self):
        """
        一時ファイル削除
        """
        # 7日以上古い一時ファイル削除
        cutoff_date = datetime.now() - timedelta(days=7)
        
        # Cloud Storage 一時バケットのクリーンアップ
        from google.cloud import storage
        
        client = storage.Client()
        bucket = client.bucket('manga-temp-data')
        
        deleted_count = 0
        for blob in bucket.list_blobs():
            if blob.time_created < cutoff_date:
                blob.delete()
                deleted_count += 1
        
        logger.info(f"Deleted {deleted_count} temporary files")
```

### 6.2 アップデート戦略

#### ローリングアップデート
```bash
#!/bin/bash
# rolling-update.sh

echo "=== AI漫画サービス ローリングアップデート ==="

# 1. 新バージョンのヘルスチェック
echo "Step 1: Health check new version"
curl -f https://staging-api.manga-service.com/health || exit 1

# 2. 段階的トラフィック移行
echo "Step 2: Gradual traffic migration"

# 10%トラフィック移行
gcloud run services update-traffic phase1-service \
  --to-revisions=LATEST=10 \
  --region=asia-northeast1

# 5分待機してメトリクス確認
sleep 300
python check_metrics.py --threshold=0.95 || exit 1

# 50%トラフィック移行
gcloud run services update-traffic phase1-service \
  --to-revisions=LATEST=50 \
  --region=asia-northeast1

sleep 300
python check_metrics.py --threshold=0.95 || exit 1

# 100%トラフィック移行
gcloud run services update-traffic phase1-service \
  --to-revisions=LATEST=100 \
  --region=asia-northeast1

# 3. 全サービス更新
for phase in {2..8}; do
  echo "Updating phase${phase}-service"
  
  gcloud run services update-traffic phase${phase}-service \
    --to-revisions=LATEST=100 \
    --region=asia-northeast1
    
  # ヘルスチェック
  sleep 60
  curl -f https://api.manga-service.com/health/phase${phase} || exit 1
done

echo "=== ローリングアップデート完了 ==="
```

### 6.3 容量管理

#### キャパシティプランニング
```python
class CapacityPlanner:
    def __init__(self):
        self.metrics_client = monitoring_v3.MetricServiceClient()
        
    async def analyze_capacity_trends(self, days: int = 30) -> dict:
        """
        容量トレンド分析
        """
        # CPU・メモリ使用率トレンド
        resource_trends = await self.get_resource_usage_trends(days)
        
        # ユーザー増加トレンド
        user_growth = await self.get_user_growth_trends(days)
        
        # API使用量トレンド
        api_usage_trends = await self.get_api_usage_trends(days)
        
        # 容量予測
        capacity_forecast = await self.forecast_capacity_needs(
            resource_trends, user_growth, api_usage_trends
        )
        
        return {
            'current_utilization': resource_trends['current'],
            'growth_rate': user_growth['monthly_growth_rate'],
            'capacity_forecast': capacity_forecast,
            'scaling_recommendations': await self.generate_scaling_recommendations(capacity_forecast)
        }
    
    async def generate_scaling_recommendations(self, forecast: dict) -> list:
        """
        スケーリング推奨事項生成
        """
        recommendations = []
        
        # CPU使用率が70%を超える予測
        if forecast['cpu_utilization_3month'] > 0.7:
            recommendations.append({
                'type': 'scale_up_compute',
                'description': 'Cloud Run CPU増強を推奨',
                'timeline': '2週間以内',
                'impact': 'パフォーマンス向上'
            })
        
        # Redis メモリ使用率が80%を超える予測
        if forecast['redis_memory_utilization_3month'] > 0.8:
            recommendations.append({
                'type': 'scale_up_redis',
                'description': 'Redis Cluster移行を推奨',
                'timeline': '1ヶ月以内',
                'impact': 'キャッシュ性能向上、高可用性確保'
            })
        
        return recommendations
```

---

## 7. 災害復旧

### 7.1 バックアップ戦略

#### 包括的バックアップ計画
```yaml
Backup Strategy:
  Database (Cloud SQL):
    Type: Automated backup + Manual snapshots
    Frequency: Daily automated, Weekly manual
    Retention: 30 days automated, 90 days manual
    Cross-region: Enabled
    
  Redis:
    Type: Daily snapshots
    Frequency: Daily at 02:00 JST
    Retention: 7 days
    Storage: Cloud Storage
    
  Application Data:
    Type: Git repository + Container images
    Frequency: Every commit
    Retention: Unlimited
    
  Configuration:
    Type: Terraform state + Kubernetes manifests
    Frequency: Every change
    Retention: Unlimited
    
  Generated Content:
    Type: Cloud Storage native replication
    Frequency: Real-time
    Retention: Based on user data policy
```

### 7.2 復旧手順

#### 災害復旧プレイブック
```bash
#!/bin/bash
# disaster-recovery-playbook.sh

echo "=== AI漫画サービス 災害復旧プレイブック ==="

# 引数チェック
if [ $# -ne 1 ]; then
    echo "Usage: $0 <disaster_type>"
    echo "disaster_type: region_outage|data_corruption|security_breach|complete_failure"
    exit 1
fi

DISASTER_TYPE=$1
RECOVERY_TIMESTAMP=$(date +%Y%m%d_%H%M%S)

case $DISASTER_TYPE in
    "region_outage")
        echo "=== リージョン障害復旧 ==="
        
        # 1. 別リージョンでの緊急立ち上げ
        TARGET_REGION="asia-northeast2"  # 大阪リージョン
        
        # データベース復旧
        gcloud sql instances clone manga-db-instance \
          manga-db-recovery-${RECOVERY_TIMESTAMP} \
          --destination-region=${TARGET_REGION}
        
        # Cloud Run サービス復旧
        for phase in {1..8}; do
            gcloud run deploy phase${phase}-service \
              --image gcr.io/${PROJECT_ID}/phase${phase}:latest \
              --region ${TARGET_REGION}
        done
        
        # DNS切り替え（手動確認後）
        echo "手動でDNS切り替えを実行してください:"
        echo "api.manga-service.com -> ${TARGET_REGION} Load Balancer"
        ;;
        
    "data_corruption")
        echo "=== データ破損復旧 ==="
        
        # 最新バックアップからの復旧
        LATEST_BACKUP=$(gcloud sql backups list --instance=manga-db-instance --limit=1 --format="value(id)")
        
        gcloud sql backups restore $LATEST_BACKUP \
          --restore-instance=manga-db-recovery-${RECOVERY_TIMESTAMP}
        
        # データ整合性確認
        python scripts/verify_data_integrity.py \
          --instance=manga-db-recovery-${RECOVERY_TIMESTAMP}
        ;;
        
    "security_breach")
        echo "=== セキュリティ侵害対応 ==="
        
        # 1. 即座のアクセス遮断
        gcloud run services update phase1-service \
          --no-allow-unauthenticated \
          --region=asia-northeast1
        
        # 2. API キー無効化
        gcloud secrets versions disable latest --secret="gemini-api-key"
        gcloud secrets versions disable latest --secret="imagen-api-key"
        
        # 3. セキュリティ監査ログ収集
        python scripts/collect_security_logs.py \
          --start-time="$(date -d '24 hours ago' -u +%Y-%m-%dT%H:%M:%SZ)" \
          --end-time="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
        ;;
        
    "complete_failure")
        echo "=== 完全障害復旧 ==="
        
        # インフラ完全再構築
        cd terraform/
        terraform init
        terraform plan -out=recovery.plan
        terraform apply recovery.plan
        
        # アプリケーション再デプロイ
        gcloud builds submit --config=cloudbuild-full-deploy.yaml
        ;;
        
    *)
        echo "Unknown disaster type: $DISASTER_TYPE"
        exit 1
        ;;
esac

echo "=== 復旧手順完了。ヘルスチェックを実行してください ==="
curl -f https://api.manga-service.com/health
```

### 7.3 事業継続計画

#### BCP（Business Continuity Plan）
```yaml
Business Continuity Plan:
  RTO (Recovery Time Objective):
    Critical Functions: 1 hour
    Important Functions: 4 hours
    Normal Functions: 24 hours
    
  RPO (Recovery Point Objective):
    User Data: 1 hour
    Generated Content: 4 hours
    System Configuration: 0 (Git管理)
    
  Continuity Strategies:
    Multi-Region Deployment:
      Primary: asia-northeast1 (Tokyo)
      Secondary: asia-northeast2 (Osaka)
      Trigger: Primary region > 30min outage
      
    Service Degradation:
      Level 1: Non-essential features disabled
      Level 2: AI generation paused, read-only mode
      Level 3: Complete maintenance mode
```

---

## 8. 運用自動化

### 8.1 自動化ツール

#### 運用自動化スタック
```yaml
Automation Tools:
  Infrastructure as Code:
    - Terraform: インフラ管理
    - Cloud Deployment Manager: GCP統合
    
  Configuration Management:
    - Cloud Build: CI/CD自動化
    - Cloud Scheduler: 定期タスク
    
  Monitoring Automation:
    - Cloud Functions: アラート対応
    - Cloud Workflows: 復旧手順自動化
    
  Data Management:
    - Cloud Composer (Airflow): データパイプライン
    - BigQuery Scheduled Queries: レポート自動生成
```

### 8.2 運用スクリプト

#### 運用スクリプト集
```python
# scripts/operational_tasks.py

class OperationalTasks:
    """
    日常運用タスクの自動化
    """
    
    async def daily_health_check(self):
        """
        日次ヘルスチェック
        """
        health_results = {}
        
        # 1. 全サービスのヘルスチェック
        services = ['phase1', 'phase2', 'phase3', 'phase4', 
                   'phase5', 'phase6', 'phase7', 'phase8']
        
        for service in services:
            health_results[service] = await self.check_service_health(service)
        
        # 2. データベース接続確認
        health_results['database'] = await self.check_database_health()
        
        # 3. Redis接続確認
        health_results['redis'] = await self.check_redis_health()
        
        # 4. 外部API接続確認
        health_results['gemini_api'] = await self.check_gemini_api_health()
        health_results['imagen_api'] = await self.check_imagen_api_health()
        
        # 5. 結果レポート生成
        report = self.generate_health_report(health_results)
        await self.send_daily_health_report(report)
        
        return health_results
    
    async def weekly_optimization(self):
        """
        週次最適化タスク
        """
        # 1. パフォーマンス分析
        performance_analysis = await self.analyze_weekly_performance()
        
        # 2. コスト分析
        cost_analysis = await self.analyze_weekly_costs()
        
        # 3. 品質トレンド分析
        quality_analysis = await self.analyze_weekly_quality()
        
        # 4. 最適化提案生成
        optimizations = await self.generate_optimization_suggestions(
            performance_analysis, cost_analysis, quality_analysis
        )
        
        # 5. 最適化実行（安全なもののみ）
        safe_optimizations = [opt for opt in optimizations if opt['risk_level'] == 'low']
        for optimization in safe_optimizations:
            await self.execute_optimization(optimization)
        
        return {
            'performance_analysis': performance_analysis,
            'cost_analysis': cost_analysis,
            'quality_analysis': quality_analysis,
            'executed_optimizations': safe_optimizations
        }
```

### 8.3 運用効率化

#### 運用ダッシュボード自動化
```python
class OperationsDashboard:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.report_generator = ReportGenerator()
        
    async def generate_operational_insights(self) -> dict:
        """
        運用インサイト自動生成
        """
        # データ収集
        system_metrics = await self.metrics_collector.collect_system_metrics()
        business_metrics = await self.metrics_collector.collect_business_metrics()
        quality_metrics = await self.metrics_collector.collect_quality_metrics()
        
        # AI による異常検出
        anomalies = await self.detect_operational_anomalies(system_metrics)
        
        # 予測分析
        forecasts = await self.generate_operational_forecasts(
            system_metrics, business_metrics
        )
        
        # 推奨アクション生成
        recommendations = await self.generate_operational_recommendations(
            anomalies, forecasts
        )
        
        return {
            'timestamp': datetime.now().isoformat(),
            'system_health': self.calculate_overall_health(system_metrics),
            'anomalies': anomalies,
            'forecasts': forecasts,
            'recommendations': recommendations,
            'sla_status': await self.calculate_current_sla()
        }
    
    async def auto_remediation(self, issue: dict):
        """
        自動修復処理
        """
        issue_type = issue['type']
        
        remediation_map = {
            'high_memory_usage': self.scale_up_memory,
            'slow_api_response': self.optimize_api_cache,
            'redis_connection_spike': self.reset_redis_connections,
            'database_slow_query': self.analyze_slow_queries
        }
        
        remediation_func = remediation_map.get(issue_type)
        if remediation_func:
            try:
                result = await remediation_func(issue)
                await self.log_auto_remediation(issue, result, success=True)
                return result
            except Exception as e:
                await self.log_auto_remediation(issue, str(e), success=False)
                # 人間にエスカレーション
                await self.escalate_to_human(issue, e)
```

---

## 9. キャパシティプランニング

### 9.1 リソース予測

#### 成長予測モデル
```python
class GrowthForecasting:
    def __init__(self):
        self.historical_data = HistoricalDataManager()
        
    async def forecast_resource_needs(self, months_ahead: int = 6) -> dict:
        """
        リソース需要予測
        """
        # 過去データ取得
        historical_metrics = await self.historical_data.get_metrics(months=6)
        
        # ユーザー成長予測
        user_forecast = self.predict_user_growth(historical_metrics['users'])
        
        # リクエスト量予測
        request_forecast = self.predict_request_volume(
            historical_metrics['requests'], user_forecast
        )
        
        # リソース使用量予測
        resource_forecast = self.predict_resource_usage(
            historical_metrics['resources'], request_forecast
        )
        
        return {
            'forecast_period': f'{months_ahead} months',
            'user_growth': user_forecast,
            'request_volume': request_forecast,
            'resource_requirements': resource_forecast,
            'scaling_timeline': self.generate_scaling_timeline(resource_forecast),
            'budget_impact': self.calculate_budget_impact(resource_forecast)
        }
    
    def predict_user_growth(self, historical_users: list) -> dict:
        """
        ユーザー成長予測（線形回帰）
        """
        import numpy as np
        from sklearn.linear_model import LinearRegression
        
        # データ準備
        X = np.array([[i] for i in range(len(historical_users))])
        y = np.array(historical_users)
        
        # 線形回帰モデル
        model = LinearRegression()
        model.fit(X, y)
        
        # 6ヶ月先まで予測
        future_months = np.array([[len(historical_users) + i] for i in range(1, 7)])
        predicted_users = model.predict(future_months)
        
        return {
            'current_users': historical_users[-1],
            'monthly_growth_rate': model.coef_[0],
            '6_month_prediction': predicted_users[-1],
            'confidence': model.score(X, y)
        }
```

### 9.2 スケーリング戦略

#### 自動スケーリング設定
```yaml
Auto Scaling Configuration:
  Cloud Run:
    Current: 1 vCPU, 2GB per service
    Trigger Conditions:
      Scale Up: CPU > 70% for 3 minutes
      Scale Down: CPU < 30% for 10 minutes
    
    Scaling Timeline:
      Month 1-3: Monitor current configuration
      Month 4-6: Scale to 2 vCPU if needed
      Month 7-12: Consider 4 vCPU for Phase 6
    
  Cloud SQL:
    Current: db-standard-1 (1 vCPU, 3.75GB)
    Trigger Conditions:
      Scale Up: CPU > 80% for 5 minutes
      Memory: > 85% for 5 minutes
    
    Scaling Path:
      Phase 1: db-standard-2 (2 vCPU, 7.5GB)
      Phase 2: db-standard-4 (4 vCPU, 15GB) + Read Replica
      Phase 3: db-highmem-8 (8 vCPU, 52GB)
    
  Redis:
    Current: Memory Store Basic 4GB
    Migration Trigger:
      - Memory usage > 80%
      - Connection count > 100
      - High availability requirement
    
    Migration Plan:
      Phase 1: Memory Store Standard 8GB
      Phase 2: Redis Cluster (3 nodes, 4GB each)
      Phase 3: Redis Cluster (6 nodes, 8GB each)
```

---

## 10. 運用チーム体制

### 10.1 役割分担

#### 運用チーム構成
```yaml
Operations Team:
  SRE Engineer:
    Primary Responsibilities:
      - SLA monitoring and enforcement
      - Incident response and resolution
      - Performance optimization
      - Reliability engineering
    
    Daily Tasks:
      - Dashboard monitoring
      - Alert triage
      - Performance analysis
      - Capacity planning
    
    Tools:
      - Cloud Monitoring
      - Cloud Logging
      - Terraform
      - Incident management system
    
  DevOps Engineer:
    Primary Responsibilities:
      - CI/CD pipeline maintenance
      - Infrastructure automation
      - Security patch management
      - Deployment coordination
    
    Daily Tasks:
      - Pipeline monitoring
      - Security updates
      - Infrastructure optimization
      - Deployment support
    
    Tools:
      - Cloud Build
      - GitHub Actions
      - Terraform
      - Container Registry
```

### 10.2 オンコール体制

#### 24/7サポート体制
```yaml
On-Call Schedule:
  Rotation: Weekly rotation
  Coverage: 24/7
  
  Response Times:
    P1 (Critical): 15 minutes
    P2 (High): 1 hour
    P3 (Medium): 4 hours during business hours
    P4 (Low): Next business day
    
  Escalation Path:
    Level 1: On-call SRE (0-30 min)
    Level 2: Tech Lead (30-60 min)
    Level 3: Engineering Manager (1-2 hours)
    Level 4: CTO (2+ hours)
    
  Handoff Process:
    - Shift summary document
    - Outstanding issues review
    - Knowledge transfer call
    - Alert acknowledgment transfer
```

### 10.3 運用手順書

#### 標準運用手順（SOP）
```python
class StandardOperatingProcedures:
    """
    標準運用手順の自動化
    """
    
    async def handle_high_error_rate(self):
        """
        高エラー率対応手順
        """
        print("=== 高エラー率対応手順開始 ===")
        
        # 1. 現状確認
        error_analysis = await self.analyze_current_errors()
        print(f"現在のエラー率: {error_analysis['error_rate']:.2%}")
        
        # 2. エラーパターン分析
        error_patterns = await self.identify_error_patterns()
        
        # 3. 影響範囲確認
        impact = await self.assess_error_impact()
        
        # 4. 自動軽減策実行
        if error_analysis['error_rate'] > 0.1:  # 10%以上
            print("自動軽減策を実行します...")
            await self.execute_error_mitigation()
        
        # 5. 手動対応が必要な場合
        if error_analysis['error_rate'] > 0.2:  # 20%以上
            await self.escalate_high_error_rate(error_analysis, impact)
        
        print("=== 高エラー率対応手順完了 ===")
    
    async def handle_performance_degradation(self):
        """
        性能劣化対応手順
        """
        print("=== 性能劣化対応手順開始 ===")
        
        # 1. パフォーマンス現状分析
        perf_metrics = await self.get_current_performance_metrics()
        
        # 2. ボトルネック特定
        bottlenecks = await self.identify_performance_bottlenecks()
        
        # 3. 自動最適化実行
        for bottleneck in bottlenecks:
            if bottleneck['auto_fixable']:
                await self.apply_performance_fix(bottleneck)
        
        # 4. 改善確認
        await asyncio.sleep(300)  # 5分後に確認
        improved_metrics = await self.get_current_performance_metrics()
        
        improvement = self.calculate_improvement(perf_metrics, improved_metrics)
        print(f"性能改善: {improvement:.1%}")
        
        print("=== 性能劣化対応手順完了 ===")
    
    async def handle_capacity_alert(self):
        """
        容量アラート対応手順
        """
        print("=== 容量アラート対応手順開始 ===")
        
        # 1. 現在のリソース使用状況確認
        resource_usage = await self.get_current_resource_usage()
        
        # 2. 緊急スケールアップ判定
        if resource_usage['requires_immediate_scaling']:
            print("緊急スケールアップを実行します...")
            await self.emergency_scale_up()
        
        # 3. 中長期キャパシティプランニング
        capacity_plan = await self.update_capacity_plan(resource_usage)
        
        # 4. チームへの通知
        await self.notify_capacity_planning_update(capacity_plan)
        
        print("=== 容量アラート対応手順完了 ===")
```

---

## 改訂履歴

| 版数 | 日付 | 変更内容 | 担当者 |
|------|------|----------|--------|
| 1.0 | 2025-01-20 | 初版作成（SLA 99.5%達成運用設計） | Claude Code |

---

**文書承認**
- SREマネージャー: [署名] 日付: [日付]
- 運用責任者: [署名] 日付: [日付]
- インフラ責任者: [署名] 日付: [日付]